{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Честная регрессия\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0000000e+00 -4.4408921e-16]\n",
      "[[-1.11022302e-16]\n",
      " [ 0.00000000e+00]]\n",
      "[ True False]\n",
      "[[False]\n",
      " [ True]]\n",
      "[1. 3. 4.]\n",
      "[[1.]\n",
      " [1.]]\n",
      "[ True  True  True]\n",
      "[[ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class TrueLinReg():\n",
    "    def __init__(self):\n",
    "        self.w_ = None  #cтолбец \n",
    "        self.coef_ = None #строка\n",
    "        \n",
    "    def fit(self, X: np.array, y: np.array) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1) #добавляем вес без признака        \n",
    "        n, m = X.shape[0], X.shape[1]\n",
    "        y = np.array(y)\n",
    "        y = y.reshape(n, 1)\n",
    "        \n",
    "        if n == m:\n",
    "            self.w_ = np.linalg.inv(X).dot(y)\n",
    "        elif n > m:\n",
    "            first = X.T.dot(X)\n",
    "            second = np.linalg.inv(first)\n",
    "            third = second.dot(X.T)\n",
    "            self.w_ = third.dot(y)\n",
    "        elif n < m:\n",
    "            first = X.dot(X.T)\n",
    "            second = np.linalg.inv(first)\n",
    "            third = X.T.dot(second)\n",
    "            self.w_ = third.dot(y)\n",
    "        \n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        return (X @ self.w_).reshape(-1)\n",
    "\n",
    "model = TrueLinReg()\n",
    "\n",
    "X_train = np.array([[1], [2], [3]])\n",
    "y_train = np.array([1, -2, 1])\n",
    "X_test = np.array([[0], [4]])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_test))\n",
    "print(model.w_)\n",
    "\n",
    "print(model.predict(X_test) == np.array([0., 0.]))\n",
    "print(model.w_ == np.array([[0.], [0.]]))\n",
    "\n",
    "X_train = np.array([[1]])\n",
    "y_train = np.array([2])\n",
    "X_test = np.array([[0.], [2.], [3.]])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_test))\n",
    "print(model.w_)\n",
    "\n",
    "print(model.predict(X_test) == np.array([1., 3., 4.]))\n",
    "print(model.w_ == np.array([[1.], [1.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Честная регуляризация\n",
    "\n",
    "Добавьте регуляризацию с коэффициентом λ и решите задачу регрессии (для любого соотношения m и n используйте формулу)\n",
    "\n",
    "$$\\omega = (X^TX+\\frac{\\lambda}{2}I)^{-1}X^TY$$\n",
    "\n",
    "где I - единичная матрица.\n",
    "\n",
    "Не забудьте сами добавить справа единичный столбец к матрице X аналогично предыдущей задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 3.        ]\n",
      "[[0.66666667]\n",
      " [0.33333333]]\n",
      "[0.66666667 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "class TrueReg():\n",
    "    def __init__(self, lamb):\n",
    "        self.lamb = lamb\n",
    "        self.w_ = None #столбец\n",
    "        self.coef_ = None #строка\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = self._add_ones_at_right(X)\n",
    "        n, m = X.shape[0], X.shape[1]\n",
    "        y = y.reshape(n, 1)\n",
    "        ones = np.eye(m)\n",
    "        \n",
    "        self.w_ = np.linalg.inv(X.T.dot(X) + self.lamb * ones).dot(X.T).dot(y)\n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = self._add_ones_at_right(X)\n",
    "        return (X @ self.w_).reshape(-1)\n",
    "    \n",
    "    def _add_ones_at_right(self, X):\n",
    "        return np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "\n",
    "    \n",
    "X_train = np.array([[1], [2]])\n",
    "y_train = np.array([1, 2])\n",
    "lamb = 1\n",
    "\n",
    "X_test = np.array([[0], [4]])\n",
    "\n",
    "model = TrueReg(lamb)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_test)) # np.array([0.33, 3])\n",
    "print(model.w_) # np.array([[0.67], [0.33]])\n",
    "print(model.coef_) # np.array([0.67, 0.33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5, 2. ],\n",
       "       [3. , 4.5]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2], [3, 4]]) + (1/2 * np.eye(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиент\n",
    "\n",
    "На вход подаются обучающая выборка (X,y) и как-то определенные веса w. Верните градиент функции MSE для изменения весов в алгоритме градиентного спуска.\n",
    "\n",
    "$$\\nabla L=\\frac{2}{n}X^T(Xw-y)$$\n",
    "\n",
    "Здесь не нужно добавлять единицы сбоку к матрице, считаем, что они уже есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.6]\n",
      " [-5.6]]\n"
     ]
    }
   ],
   "source": [
    "def gradient_step(w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    n = X.shape[0]\n",
    "    w = w.reshape(w.shape[0], 1)\n",
    "    return (2 / n) * X.T.dot(X.dot(w) - y)\n",
    "\n",
    "w = np.array([0.2, 0.2])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[2],[3]]) #столбец!\n",
    "print(gradient_step(w, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стохастический градиент\n",
    "\n",
    "Реализуйте стохаистический градиентный спуск: пересчитайте $$\\nabla_{w}L$$ только для одного случайно выбранного элемента, а остальные элементы вектора пусть будут равны нулю.\n",
    "\n",
    "$$\\nabla_{w_j}L=\\frac{2}{n} \\sum_{i=1}^{n}x_{i, j}(< x_i, w > -y_i)$$\n",
    "\n",
    "Здесь не нужно добавлять единицы сбоку к матрице X, считаем, что они уже есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def stochastic_step(w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    n = X.shape[0]\n",
    "    r = random.randint(0, n-1)\n",
    "    rand_X = X[r]\n",
    "    return 2 * rand_X * (rand_X.dot(w) - y[r])\n",
    "    \n",
    "\n",
    "w = np.array([[1], [1], [1]])\n",
    "X = np.array([[1, 1, 1], [0, 0, 0]])\n",
    "y = np.array([[1], [0]])\n",
    "\n",
    "print(stochastic_step(w, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск\n",
    "\n",
    "Теперь реализуем Линейную регрессию на градиентном спуске. Реализуйте пересчёт весов в цикле для алгоритма градиентного спуска. Начальные веса инициализированы нулями.\n",
    "$$w_{new}=w - \\eta \\nabla L$$\n",
    "Вы можете использовать из предыдущей задачи как простой градиентный спуск, так и стохастический:\n",
    "\n",
    "Для простого лучше использовать параметры max_iter=1000, eta=0.1\n",
    "Для стохастического: max_iter=100000, eta=0.05\n",
    "В случае стохастического градиентного спуска есть веротяность не сойтись, в этом случае задача будет не зачтена. Попробуйте перезаслать несколько раз (если 5 раз не хватило, значит ошибка где-то в коде).\n",
    "\n",
    "Подсказка: не забудьте остановиться если вы достигли минимума (разница между старыми и новыми весами - крайне мала)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.99999984 3.99999973]\n",
      "[9.99999886e-01 1.85168395e-07]\n"
     ]
    }
   ],
   "source": [
    "class GDLinReg():\n",
    "    def __init__(self, max_iter=1000, eta=0.1):\n",
    "        self.max_iter = max_iter\n",
    "        self.eta = eta\n",
    "        self.coef_ = None #строка\n",
    "        self.w_ = None #столбец\n",
    "    \n",
    "    def _gradient_descending(self, w, X, y):\n",
    "        n = X.shape[0]\n",
    "        for i in range(self.max_iter):\n",
    "            self.w_ -= self.eta * (2 / n) * X.T.dot(X.dot(self.w_) - y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        n, m = X.shape\n",
    "        self.w_ = np.zeros((m, 1)) #столбец\n",
    "        y = y.reshape(y.shape[0], 1)\n",
    "        \n",
    "        self._gradient_descending(w, X, y)\n",
    "        \n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)   \n",
    "        return (X @ self.w_).reshape(-1) #возвращаем всегда строку\n",
    "    \n",
    "X_train = np.array([[1], [2]])\n",
    "y_train = np.array([1, 2])\n",
    "\n",
    "model = GDLinReg(max_iter=1000, eta=0.1).fit(X_train, y_train)\n",
    "y_pred = model.predict(np.array([[3],[4]]))\n",
    "print(y_pred) #np.array([3., 4.])\n",
    "print(model.coef_) # np.array([1., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регуляризация\n",
    "\n",
    "Реализуйте L2-регуляризацию (так же известную как Ridge).\n",
    "\n",
    "$$L = \\frac{1}{n} \\|Xw-y\\|^2_2+\\frac{\\lambda}{2} \\|w\\|_2^2$$\n",
    "​\t\n",
    " \n",
    "Найдите новый $$\\nabla_{w}L$$ и верните его.\n",
    "\n",
    "Обратите внимание, что свободный коэффициент весов (самый последний) не нужно регуляризовывать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19.]\n",
      " [18.]]\n"
     ]
    }
   ],
   "source": [
    "def gradient_step_l2(w: np.array, X: np.array, y: np.array, lamb: float) -> np.array:\n",
    "    n = X.shape[0]\n",
    "    w = w.reshape(w.shape[0], 1)\n",
    "    w_without_last = np.copy(w)\n",
    "    w_without_last[w.shape[0] - 1, 0] = 0\n",
    "    return (2 / n) * X.T.dot(X.dot(w) - y) + lamb * w_without_last\n",
    "    \n",
    "w = np.array([[1], [1]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[0],[0]])\n",
    "\n",
    "print(gradient_step_l2(w, X, y, 1)) # np.array([[19.], [18.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистический градиент\n",
    "\n",
    "Теперь переходим к задаче классификации и Логистической регрессии. Мы уже знаем, что в нашем случае нужно минимизировать метрику LogLoss:\n",
    "\n",
    "$$lnL=\\sum_{i=1}^{n}y_iln(\\sigma(x_i^Tw)) + (1-y_i)ln(1-\\sigma(x_i^Tw))$$, где\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}} y \\in \\{0,1\\}$$\n",
    "\n",
    "Ваша задача взять производную этой функции и вернуть вектор градиентов $\\nabla_wlnL$\n",
    "\n",
    "Формулу необходимо вывести в векторном виде, чтобы считалось быстрее.\n",
    "\n",
    "Обратите внимание:\n",
    "\n",
    "- w, y - столбцы, а не строки. Ответ тоже столбец.\n",
    "- В функции используется натуральный логарифм.\n",
    "- Нужно посчитать полный градиент, а не стохастический."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-14729474bdbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mlog_gradient_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-135-14729474bdbc>\u001b[0m in \u001b[0;36mlog_gradient_step\u001b[0;34m(w, X, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mefv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mefv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def ef(v):\n",
    "    return math.exp(v)\n",
    "\n",
    "def log_gradient_step(w: np.array, X: np.array, y: np.array)-> np.array:\n",
    "    w = np.array(w)\n",
    "    w = w.reshape(w.shape[0], 1)\n",
    "    efv = np.vectorize(ef)\n",
    "    e = efv(X.dot(w))\n",
    "    return X.T.dot((1 - (e / (1 + e)) - y))\n",
    "    \n",
    "    return (y - (1-y) * e) / (1 + e)\n",
    "    \n",
    "w = np.array([0, 0])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[0],[0]])\n",
    "log_gradient_step(w, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
