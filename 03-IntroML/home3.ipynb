{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Первое обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простое как пробка задание. Обучите классификатор [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) на входных данных с гиперпараметрами:\n",
    "* max_depth=6\n",
    "* min_samples_split=3\n",
    "* min_samples_leaf=3\n",
    "* n_estimators=100\n",
    "* n_jobs=-1\n",
    "\n",
    "И верните обученную модель.\n",
    "\n",
    "Данные в X только численные, в y только 2 значения: 0 и 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def fit_rf(X: np.ndarray, y:np.ndarray) ->  RandomForestClassifier:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) Первая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По [ссылке](https://yadi.sk/d/xhbGrNuOjv6bjA) вы можете найти данные для бинарной классификации. $Y$ в этих данных выступает столбик `Outcome`, в качаестве $X$ - все остальное. \n",
    "\n",
    "Вам необходимо предсказать $y_{test}$ такой, что $accuracy > 0.75$ ([доля правильных ответов](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)). Вы можете делать что угодно получить результат:\n",
    "* использовать любой классификатор с любыми гиперпараметрами\n",
    "* как угодно изменять данные \n",
    "\n",
    "Вернуть в этом случае нужно не модель, а результат - одномерный массив данных $y_{pred}$ (предсказание $y_{test}$).\n",
    "\n",
    "P.S. Можете узнать больше о данных по [ссылке](https://www.kaggle.com/uciml/pima-indians-diabetes-database). Мы произвольным образом разбили данные в соотношении 4:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def classification(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) Переобучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По [ссылке](https://yadi.sk/d/BtzKJULWGf0nng) вы можете найти данные для бинарной классификации. Вам на вход подается тренировачная и тестовая выборки из файла. Верните такую обученную модель, которая на тренировачной выборке дает $accuracy > 0.97$, а на тестовом $accuracy < 0.7$.\n",
    "\n",
    "[$accuracy$](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) - доля правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfitting(X_train: np.array, y_train: np.array, X_test: np.array, y_test: np.array):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (3) Мой KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже научились обучать разные задачи, теперь давайте напишем свой классификатор. Ваша задача реализовать свой простой KNN. Вам нужно реализовать 3 метода:\n",
    "* `init` - начальная инициализация\n",
    "* `fit` - обучение классификатора\n",
    "* `predict` - предсказание для новых объектов\n",
    "* `predict_proba` - предсказание вероятностей новых объектов\n",
    "\n",
    "У нашего классификатора будет лишь один гиперпараметр - количество соседей $k$. Во избежании тонкостей: $k$ - нечетное.\n",
    "\n",
    "На вход будет подаваться выборка объектов $X$, у которых ровно 2 числовых признака. $y$ - результат бинарной классификации $0$ или $1$.\n",
    "\n",
    "Метрика ближайших элементов - Эвклидова.\n",
    "\n",
    "Напоминание: $y$ - одномерный массив, $X$ - двумерный массив, по $0$-ой оси которой расположены объекты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad predicted  0\n",
      "[[0.  1. ]\n",
      " [0.2 0.8]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [1.  0. ]]\n",
      "[[0.  1. ]\n",
      " [0.2 0.8]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [0.  1. ]\n",
      " [1.  0. ]]\n",
      "bad predicted  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "class MyKNN():\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.train_points = {}\n",
    "        \n",
    "    def fit(self, X_train: np.array, y_train:np.array): #обучаем классификатор\n",
    "        n = X_train.shape[0]\n",
    "        for i in range(n):\n",
    "            point = tuple(X_train[i])\n",
    "            res = y_train[i]\n",
    "            self.train_points[point] = res\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test: np.array): #возвращаем значения\n",
    "        result = []\n",
    "        n = X_test.shape[0]\n",
    "        for i in range(n):\n",
    "            point = tuple(X_test[i])\n",
    "            neighbors = self._get_neighbors(point)\n",
    "            res = self._get_more_freq_res(neighbors)\n",
    "            result.append(res)\n",
    "        return np.array(result)           \n",
    "                \n",
    "    \n",
    "    def predict_proba(self, X_test: np.array): #возвращаем значения\n",
    "        result = []\n",
    "        n = X_test.shape[0]\n",
    "        for i in range(n):\n",
    "            point = tuple(X_test[i])\n",
    "            neighbors = self._get_neighbors(point)\n",
    "            res = self._get_proba(neighbors)\n",
    "            result.append(res)\n",
    "        return np.array(result) \n",
    "    \n",
    "    def _get_distance(self, point1, point2):\n",
    "        return pow(point1[0] - point2[0], 2) + pow(point1[1] - point2[1], 2)\n",
    "    \n",
    "    def _get_neighbors(self, point):\n",
    "        neighbors = {}\n",
    "        max_dist = 0\n",
    "        for train_point in self.train_points.keys():\n",
    "            dist = self._get_distance(point, train_point)\n",
    "            if len(neighbors) < self.k:\n",
    "                neighbors[train_point] = dist\n",
    "                max_dist = max(max_dist, dist)\n",
    "            elif dist < max_dist:\n",
    "                for p, d in neighbors.items():\n",
    "                    if d == max_dist:\n",
    "                        del neighbors[p]\n",
    "                        break\n",
    "                neighbors[train_point] = dist\n",
    "                max_dist = max(neighbors.values())\n",
    "        return neighbors.keys()\n",
    "    \n",
    "    def _get_more_freq_res(self, neighbors):\n",
    "        zeros_count, ones_count = self._get_zeros_ones_count(neighbors)\n",
    "        return 0 if zeros_count > ones_count else 1\n",
    "    \n",
    "    def _get_proba(self, neighbors):\n",
    "        zeros, ones = self._get_zeros_ones_count(neighbors)\n",
    "        return zeros / (zeros + ones), ones / (zeros + ones) \n",
    "        \n",
    "    def _get_zeros_ones_count(self, neighbors):\n",
    "        zeros_count = 0\n",
    "        ones_count = 0\n",
    "        for train_point in neighbors:\n",
    "            res = self.train_points[train_point]\n",
    "            if res == 0:\n",
    "                zeros_count += 1 \n",
    "            else:\n",
    "                ones_count += 1\n",
    "        return zeros_count, ones_count\n",
    "        \n",
    "\n",
    "'''\n",
    "X_train = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_train = np.array([1, 1, 0, 0])\n",
    "model = MyKNN(k=3).fit(X_train, y_train)\n",
    "y_pred = model.predict(np.array([[0.5, 0.5], [ -0.5,  -0.5]]))\n",
    "y_prob = model.predict_proba(np.array([[0.5, 0.5], [-0.5, -0.5]]))\n",
    "print(y_pred)\n",
    "print(y_prob)\n",
    "real = KNN(3).fit(X_train, y_train)\n",
    "print(real.predict(np.array([[0.5, 0.5], [ -0.5,  -0.5]])))\n",
    "print(real.predict_proba(np.array([[0.5, 0.5], [-0.5, -0.5]])))\n",
    "'''\n",
    "\n",
    "X_train, y_train = make_classification(n_features=2, n_classes=2, n_samples=500, \n",
    "                                       n_informative=2, n_redundant=0, n_repeated=0)\n",
    "X_test, _ = make_classification(n_features=2, n_classes=2, n_samples=500, \n",
    "                                n_informative=2, n_redundant=0, n_repeated=0)\n",
    "\n",
    "k = 5\n",
    "my_clf = MyKNN(k)\n",
    "clf = KNN(k)\n",
    "my_clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "my_predict = my_clf.predict(X_test)\n",
    "real_predict = clf.predict(X_test)\n",
    "bad = 0\n",
    "for i in range(my_predict.shape[0]):\n",
    "    if my_predict[i] != real_predict[i]:\n",
    "        bad += 1\n",
    "print('bad predicted ', bad)\n",
    "\n",
    "my_predict_proba = my_clf.predict_proba(X_test)\n",
    "print(my_predict_proba[:10])\n",
    "real_predict_proba = clf.predict_proba(X_test)\n",
    "print(real_predict_proba[:10])\n",
    "bad = 0\n",
    "for i in range(my_predict.shape[0]):\n",
    "    if (my_predict_proba[i][0] != real_predict_proba[i][0]) or \\\n",
    "    (my_predict_proba[i][1] != real_predict_proba[i][1]):\n",
    "        bad += 1\n",
    "print('bad predicted ', bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) Моя Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вам предстоит реализовать свою простейшую линейную регрессию по функкционалу $MSE$.\n",
    "\n",
    "Линейная регрессия выглядит следующим образом:\n",
    "$$a(x) = w_1x + w_0$$\n",
    "\n",
    "Необходимо найти такие $w_0$ и $w_1$ при которых минимизируется значение\n",
    "\n",
    "$$MSE(X,Y) = \\sum_{i=1}^{n}(a(x_i) - y_i)^2$$\n",
    "\n",
    "Выведите формулы для $w_0$ и $w_1$ аналитически и реализуйте следующие методы класса \n",
    "\n",
    "* `init` - начальная инициализация\n",
    "* `fit` - обучение классификатора\n",
    "* `predict` - предсказание для новых объектов\n",
    "\n",
    "После обучения у модели должен присутствовать атрибут `model.coef_` из которого можно получить коэффициенты регрессии в порядке: $w_1$, $w_0$.\n",
    "\n",
    "Гиперпараметры отсутствуют\n",
    "\n",
    "На вход будут подаваться два массива $X\\in \\mathbb{R}^{n}$ и $Y \\in \\mathbb{R}^{n}$\n",
    "\n",
    "Метрика - Эвклидова.\n",
    "\n",
    "Подсказка: для тестирования можете использовать реальную `LinearRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.  8.5]\n",
      "[2.5]\n",
      "[6.  8.5]\n",
      "[2.500000000000002, -1.5000000000000062]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "class LinReg():\n",
    "    def __init__(self):\n",
    "        self.coef_ = []\n",
    "\n",
    "    def fit(self, X_train: np.array, y_train: np.array): #обучаем классификатор\n",
    "        y_mean = np.mean(y_train)\n",
    "        x_mean = np.mean(X_train)\n",
    "        x_sqr_mean = np.mean(X_train * X_train)\n",
    "        prod = []\n",
    "        for i in range(y_train.shape[0]):\n",
    "            prod.append(X_train[i] * y_train[i])\n",
    "        prod_mean = np.mean(prod)\n",
    "        b = (prod_mean - y_mean * x_mean) / (x_sqr_mean - x_mean * x_mean)\n",
    "        a = y_mean - b * x_mean\n",
    "        self.coef_ = [b, a]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test: np.array): #возвращаем значения\n",
    "        b, a = self.coef_[0], self.coef_[1]\n",
    "        f = np.vectorize(lambda x: b * x + a)\n",
    "        return f(X_test).flatten()\n",
    "    \n",
    "X_train = np.array([[1], [2], [4]])\n",
    "y_train = np.array([0, 5, 8])\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = lr.predict(np.array([[3], [4]]))\n",
    "print(y_pred)\n",
    "print(lr.coef_)\n",
    "\n",
    "model = LinReg().fit(X_train, y_train)\n",
    "y_pred = model.predict(np.array([[3], [4]]))\n",
    "print(y_pred)\n",
    "print(model.coef_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
