{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Энтропия и критерий Джини\n",
    "\n",
    "$p_i$ - вероятность нахождения системы в i-ом состоянии.\n",
    "\n",
    "Энтропия Шеннона определяется для системы с N возможными состояниями следующим образом\n",
    "\n",
    "$S = - \\sum_{i=1}^Np_ilog_2p_i$\n",
    "\t \n",
    "Критерий Джини (Gini Impurity). Максимизацию этого критерия можно интерпретировать как максимизацию числа пар объектов одного класса, оказавшихся в одном поддереве.\n",
    "\n",
    "В общем случае критерий Джини считается как\n",
    "$G = 1 - \\sum_k(p_k)^2$\n",
    " \n",
    "Необходимо посчитать, значения Энтропии и критерия Джини"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.971, 0.48)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def get_possibilities(y):\n",
    "    count = len(y)\n",
    "    uniq_values = set(y)\n",
    "    possibilities = []\n",
    "    for value in uniq_values:\n",
    "        possibilities.append(len(y[y ==value]) / count)\n",
    "    return possibilities\n",
    "\n",
    "def gini_impurity(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * p\n",
    "    return round(1 - sum, 3)\n",
    "\n",
    "def entropy(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * math.log2(p)\n",
    "    return round(-sum, 3)\n",
    "\n",
    "def calc_criteria(y: np.ndarray) -> (float, float):\n",
    "    assert y.ndim == 1\n",
    "    return entropy(y), gini_impurity(y)\n",
    "\n",
    "y = np.array([1,1,1,1,1,1,0,0,0,0])\n",
    "calc_criteria(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information gain\n",
    "Вам надо реализовать функцию inform_gain, которая будет вычислять прирост информации для критерия (энтропия или критерий Джини) при разбиении выбрки по признаку (threshold).\n",
    "\n",
    "Прирост информации при разбиении выборки по признаку Q (например x≤12) определяется как\n",
    "\n",
    "$IG(Q)=S_0- \\sum_{i=1}^q\\frac{N_i}{N}S_i$\t\n",
    " \n",
    "где q - число групп после разбиения. $N_i$ - число элементов выборки, у которых признак Q имеет i-ое значение.\n",
    "\n",
    "И написать функцию get_best_threshold, которая будет находить наилучшее разбиение выборки.\n",
    "\n",
    "На вход подается:\n",
    "\n",
    "- X - одномерный массив - значения признака.\n",
    "- y - значения бинарных классов.\n",
    "- criteria_func - функция критерия, для которой вычислется наилучшее разбиение (Добавлять код из предыдущей задачи не нужно, мы сами передадим нужную функцию).\n",
    "- thr - значение разбиения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 1.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def get_possibilities(y):\n",
    "    count = len(y)\n",
    "    y = list(y)\n",
    "    uniq_values = set(y)\n",
    "    possibilities = []\n",
    "    for value in uniq_values:\n",
    "        possibilities.append(len(list(filter(lambda x: x == value, y))) / count)\n",
    "    return possibilities\n",
    "\n",
    "def gini_impurity(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * p\n",
    "    return round(1 - sum, 3)\n",
    "\n",
    "def entropy(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * math.log2(p)\n",
    "    return round(-sum, 3)\n",
    "\n",
    "def len_check_criteria_func(arr, criteria_func):\n",
    "    if len(arr) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return criteria_func(arr)\n",
    "\n",
    "def inform_gain(X: np.ndarray, y: np.ndarray, threshold: float, criteria_func) -> float:\n",
    "    s0 = criteria_func(y)\n",
    "    count = y.shape[0]\n",
    "    first = []\n",
    "    second = []\n",
    "    for i in range(count):\n",
    "        if X[i] <= threshold:\n",
    "            first.append(y[i])\n",
    "        else:\n",
    "            second.append(y[i])\n",
    "    s1 = len_check_criteria_func(first, criteria_func)\n",
    "    s2 = len_check_criteria_func(second, criteria_func)\n",
    "    return s0 - len(first) / count * s1 - len(second) / count * s2\n",
    "            \n",
    "\n",
    "def get_best_threshold(X: np.ndarray, y: np.ndarray, criteria_func) -> (float, float):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    uniq_values = set(X)\n",
    "    for value in uniq_values:\n",
    "        score = inform_gain(X, y, value, criteria_func)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = value\n",
    "    return best_threshold, best_score\n",
    "\n",
    "X = np.array([3, 9, 0, 4, 7, 2, 1, 6, 8, 5])\n",
    "y = np.array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1])\n",
    "threshold=3\n",
    "criteria_func=entropy\n",
    "print(inform_gain(X, y, threshold, criteria_func))\n",
    "\n",
    "X = np.array([3, 9, 0, 4, 7, 2, 1, 6, 8, 5])\n",
    "y = np.array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1])\n",
    "criteria_func=entropy\n",
    "get_best_threshold(X, y, criteria_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6099865470109875\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(1 -(-(5/6) * math.log2(5/6) - (1/6)* math.log2(1/6))* 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best split\n",
    "\n",
    "Реализуйте функцию find_best_split, которая находит наилучшее разбиение по всем признакам. На вход подется обучающая выборка и функция критерий. Необходимо вернуть: индекс фичи, значение границы (threshold) и результат разбиение (information gain).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -1, 1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_possibilities(y):\n",
    "    count = len(y)\n",
    "    y = list(y)\n",
    "    uniq_values = set(y)\n",
    "    possibilities = []\n",
    "    for value in uniq_values:\n",
    "        possibilities.append(len(list(filter(lambda x: x == value, y))) / count)\n",
    "    return possibilities\n",
    "\n",
    "def gini_impurity(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * p\n",
    "    return round(1 - sum, 3)\n",
    "\n",
    "def entropy(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * math.log2(p)\n",
    "    return round(-sum, 3)\n",
    "\n",
    "def inform_gain(X: np.ndarray, y: np.ndarray, threshold: float, criteria_func) -> float:\n",
    "    s0 = criteria_func(y)\n",
    "    count = y.shape[0]\n",
    "    first = []\n",
    "    second = []\n",
    "    for i in range(count):\n",
    "        if X[i] <= threshold:\n",
    "            first.append(y[i])\n",
    "        else:\n",
    "            second.append(y[i])\n",
    "    s1 = criteria_func(first)\n",
    "    s2 = criteria_func(second)\n",
    "    return s0 - len(first) / count * s1 - len(second) / count * s2\n",
    "            \n",
    "\n",
    "def get_best_threshold(X: np.ndarray, y: np.ndarray, criteria_func) -> (float, float):\n",
    "    assert X.ndim == 1\n",
    "    assert y.ndim == 1\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    uniq_values = set(X)\n",
    "    for value in uniq_values:\n",
    "        score = inform_gain(X, y, value, criteria_func)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = value\n",
    "    return best_threshold, best_score\n",
    "\n",
    "def find_best_split(X, y, criteria_func):\n",
    "    assert X.ndim == 2\n",
    "    assert y.ndim == 1\n",
    "    best_feature = 0\n",
    "    best_score = 0\n",
    "    best_threshold = 0\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        feature_column = X[:, i]\n",
    "        threshold, score = get_best_threshold(feature_column, y, criteria_func)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_feature = i\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_feature, best_threshold, best_score\n",
    "\n",
    "X = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y = np.array([1, 1, 0, 0])\n",
    "criteria_func=entropy\n",
    "find_best_split(X, y, criteria_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мое дерево решений\n",
    "\n",
    "Ваша задача реализовать свой простой KNNClassifier для бинарных данных. Вам нужно реализовать 3 метода:\n",
    "\n",
    "fit - обучение классификатора\n",
    "predict - предсказание для новых объектов\n",
    "predict_proba - предсказание вероятностей новых объектов\n",
    "У нашего классификатора будет лишь два гиперпараметра - максимальная глубина дерева max_depth и критерий разбиения criterion. Энтропия или Джини.\n",
    "\n",
    "Все функции из предыдущих заданий нужно добавить в этот код.\n",
    "\n",
    "На вход будет подаваться выборка объектов X. y - результат бинарной классификации 0 или 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X  [ 1  2 -1 -1  2  3]\n",
      "threshold  1\n",
      "y  [1 1 0 0 0 0]\n",
      "first  [0, 0]\n",
      "second  [1, 1, 0, 0]\n",
      "value  1  score  0.2513333333333334\n",
      "X  [ 1  2 -1 -1  2  3]\n",
      "threshold  2\n",
      "y  [1 1 0 0 0 0]\n",
      "first  [1, 0, 0]\n",
      "second  [1, 0, 0]\n",
      "value  2  score  0.0\n",
      "X  [ 1  2 -1 -1  2  3]\n",
      "threshold  3\n",
      "y  [1 1 0 0 0 0]\n",
      "first  [1, 1, 0, 0, 0]\n",
      "second  [0]\n",
      "value  3  score  0.10883333333333334\n",
      "X  [ 1  2 -1 -1  2  3]\n",
      "threshold  -1\n",
      "y  [1 1 0 0 0 0]\n",
      "first  []\n",
      "second  [1, 1, 0, 0, 0, 0]\n",
      "value  -1  score  0.0\n",
      "X  [ 1  2 -1 -1  2  3]\n",
      "y  [1 1 0 0 0 0]\n",
      "column  0  threshold  1  score  0.2513333333333334\n",
      "X  [ 1 -1 -1 -4  3  1]\n",
      "threshold  1\n",
      "y  [1 1 0 0 0 0]\n",
      "first  [1, 0, 0]\n",
      "second  [1, 0, 0]\n",
      "value  1  score  0.0\n",
      "X  [ 1 -1 -1 -4  3  1]\n",
      "threshold  3\n",
      "y  [1 1 0 0 0 0]\n",
      "first  [1, 1, 0, 0, 0]\n",
      "second  [0]\n",
      "value  3  score  0.10883333333333334\n",
      "X  [ 1 -1 -1 -4  3  1]\n",
      "threshold  -4\n",
      "y  [1 1 0 0 0 0]\n",
      "first  []\n",
      "second  [1, 1, 0, 0, 0, 0]\n",
      "value  -4  score  0.0\n",
      "X  [ 1 -1 -1 -4  3  1]\n",
      "threshold  -1\n",
      "y  [1 1 0 0 0 0]\n",
      "first  [0]\n",
      "second  [1, 1, 0, 0, 0]\n",
      "value  -1  score  0.10883333333333334\n",
      "X  [ 1 -1 -1 -4  3  1]\n",
      "y  [1 1 0 0 0 0]\n",
      "column  1  threshold  3  score  0.10883333333333334\n",
      "X  [-1 -1]\n",
      "threshold  -1\n",
      "y  [0 0]\n",
      "first  []\n",
      "second  [0, 0]\n",
      "value  -1  score  0.0\n",
      "X  [-1 -1]\n",
      "y  [0 0]\n",
      "column  0  threshold  None  score  None\n",
      "X  [-1 -4]\n",
      "threshold  -4\n",
      "y  [0 0]\n",
      "first  []\n",
      "second  [0, 0]\n",
      "value  -4  score  0.0\n",
      "X  [-1 -4]\n",
      "threshold  -1\n",
      "y  [0 0]\n",
      "first  [0]\n",
      "second  [0]\n",
      "value  -1  score  0.0\n",
      "X  [-1 -4]\n",
      "y  [0 0]\n",
      "column  1  threshold  None  score  None\n",
      "X  [1 2 2 3]\n",
      "threshold  1\n",
      "y  [1 1 0 0]\n",
      "first  []\n",
      "second  [1, 1, 0, 0]\n",
      "value  1  score  0.0\n",
      "X  [1 2 2 3]\n",
      "threshold  2\n",
      "y  [1 1 0 0]\n",
      "first  [1]\n",
      "second  [1, 0, 0]\n",
      "value  2  score  0.3115\n",
      "X  [1 2 2 3]\n",
      "threshold  3\n",
      "y  [1 1 0 0]\n",
      "first  [1, 1, 0]\n",
      "second  [0]\n",
      "value  3  score  0.3115\n",
      "X  [1 2 2 3]\n",
      "y  [1 1 0 0]\n",
      "column  0  threshold  2  score  0.3115\n",
      "X  [ 1 -1  3  1]\n",
      "threshold  1\n",
      "y  [1 1 0 0]\n",
      "first  [1]\n",
      "second  [1, 0, 0]\n",
      "value  1  score  0.3115\n",
      "X  [ 1 -1  3  1]\n",
      "threshold  3\n",
      "y  [1 1 0 0]\n",
      "first  [1, 1, 0]\n",
      "second  [0]\n",
      "value  3  score  0.3115\n",
      "X  [ 1 -1  3  1]\n",
      "threshold  -1\n",
      "y  [1 1 0 0]\n",
      "first  []\n",
      "second  [1, 1, 0, 0]\n",
      "value  -1  score  0.0\n",
      "X  [ 1 -1  3  1]\n",
      "y  [1 1 0 0]\n",
      "column  1  threshold  1  score  0.3115\n",
      "X  [1]\n",
      "threshold  1\n",
      "y  [1]\n",
      "first  []\n",
      "second  [1]\n",
      "value  1  score  0.0\n",
      "X  [1]\n",
      "y  [1]\n",
      "column  0  threshold  None  score  None\n",
      "X  [1]\n",
      "threshold  1\n",
      "y  [1]\n",
      "first  []\n",
      "second  [1]\n",
      "value  1  score  0.0\n",
      "X  [1]\n",
      "y  [1]\n",
      "column  1  threshold  None  score  None\n",
      "X  [2 2 3]\n",
      "threshold  2\n",
      "y  [1 0 0]\n",
      "first  []\n",
      "second  [1, 0, 0]\n",
      "value  2  score  0.0\n",
      "X  [2 2 3]\n",
      "threshold  3\n",
      "y  [1 0 0]\n",
      "first  [1, 0]\n",
      "second  [0]\n",
      "value  3  score  0.2513333333333334\n",
      "X  [2 2 3]\n",
      "y  [1 0 0]\n",
      "column  0  threshold  3  score  0.2513333333333334\n",
      "X  [-1  3  1]\n",
      "threshold  1\n",
      "y  [1 0 0]\n",
      "first  [1]\n",
      "second  [0, 0]\n",
      "value  1  score  0.918\n",
      "X  [-1  3  1]\n",
      "threshold  3\n",
      "y  [1 0 0]\n",
      "first  [1, 0]\n",
      "second  [0]\n",
      "value  3  score  0.2513333333333334\n",
      "X  [-1  3  1]\n",
      "threshold  -1\n",
      "y  [1 0 0]\n",
      "first  []\n",
      "second  [1, 0, 0]\n",
      "value  -1  score  0.0\n",
      "X  [-1  3  1]\n",
      "y  [1 0 0]\n",
      "column  1  threshold  1  score  0.918\n",
      "{'cond': (0, 1), 'leaf': False, 'left': {'cond': 0, 'leaf': True}, 'right': {'cond': (0, 2), 'leaf': False, 'left': {'cond': 1, 'leaf': True}, 'right': {'cond': 0, 'leaf': True, 'proba': [0.6666666666666666, 0.3333333333333333]}}}\n",
      "[0, 0]\n",
      "[[0.6666666666666666, 0.3333333333333333], [1.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MyDecisionTreeClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=4, criterion='entropy'): \n",
    "        self.eps = 0.001\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion # 'entropy' or 'gini' \n",
    "        self.tree = {}\n",
    "        self._criteria_func = {\n",
    "            'gini': self._gini_impurity,\n",
    "            'entropy': self._entropy\n",
    "        }\n",
    "        \n",
    "    def _get_possibilities(self, y):\n",
    "        count = len(y)\n",
    "        y = list(y)\n",
    "        uniq_values = set(y)\n",
    "        possibilities = []\n",
    "        for value in uniq_values:\n",
    "            possibilities.append(len(list(filter(lambda x: x == value, y))) / count)\n",
    "        return possibilities\n",
    "\n",
    "    def _entropy(self, y: np.ndarray) -> float:\n",
    "        possibilities = self._get_possibilities(y)\n",
    "        sum = 0\n",
    "        for p in possibilities:\n",
    "            sum += p * math.log2(p)\n",
    "        return round(-sum, 3)\n",
    "\n",
    "    def _gini_impurity(self, y: np.ndarray) -> float:\n",
    "        possibilities = self._get_possibilities(y)\n",
    "        sum = 0\n",
    "        for p in possibilities:\n",
    "            sum += p * p\n",
    "        return round(1 - sum, 3)\n",
    "    \n",
    "    def _inform_gain(self, X: np.ndarray, y: np.ndarray, threshold: float, criteria_func) -> float:\n",
    "        print('X ', X)\n",
    "        print('threshold ', threshold)\n",
    "        print('y ', y)\n",
    "        s0 = criteria_func(y)\n",
    "        count = y.shape[0]\n",
    "        first = []\n",
    "        second = []\n",
    "        for i in range(count):\n",
    "            if X[i] < threshold - self.eps:\n",
    "                first.append(y[i])\n",
    "            else:\n",
    "                second.append(y[i])\n",
    "        print('first ', first)\n",
    "        print('second ', second)\n",
    "        s1 = criteria_func(first)\n",
    "        s2 = criteria_func(second)\n",
    "        return s0 - len(first) / count * s1 - len(second) / count * s2\n",
    "    \n",
    "    def _get_best_threshold(self, X: np.ndarray, y: np.ndarray, criteria_func) -> (float, float):\n",
    "        found_bigger_score = False\n",
    "        best_threshold = 0\n",
    "        best_score = 0\n",
    "        uniq_values = set(X)\n",
    "        for value in uniq_values:\n",
    "            score = self._inform_gain(X, y, value, criteria_func)\n",
    "            print('value ', value, ' score ', score)\n",
    "            if score > best_score:\n",
    "                found_bigger_score = True\n",
    "                best_score = score\n",
    "                best_threshold = value\n",
    "        if found_bigger_score:\n",
    "            return best_threshold, best_score\n",
    "        return None, None\n",
    "    \n",
    "    def _find_best_split(self, X, y, criteria_func):\n",
    "        best_feature = 0\n",
    "        best_score = 0\n",
    "        best_threshold = 0\n",
    "        found_best = False\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            feature_column = X[:, i]\n",
    "            threshold, score = self._get_best_threshold(feature_column, y, criteria_func)\n",
    "            \n",
    "            print('X ', feature_column)\n",
    "            print('y ', y)\n",
    "            print('column ', i, ' threshold ', threshold, ' score ', score)\n",
    "            \n",
    "            if score is None: \n",
    "                continue\n",
    "            \n",
    "            if score > best_score:\n",
    "                found_best = True\n",
    "                best_score = score\n",
    "                best_feature = i\n",
    "                best_threshold = threshold\n",
    "        if found_best:\n",
    "            return best_feature, best_threshold\n",
    "        return None, None\n",
    "    \n",
    "    \n",
    "    def _get_biggest_class(self, y):\n",
    "        y = list(y)\n",
    "        return max(set(y), key = y.count)\n",
    "    \n",
    "    def _get_probs(self, y):\n",
    "        count = y.shape[0]\n",
    "        ones_count = np.count_nonzero(y == 1)\n",
    "        null_count = count - ones_count\n",
    "        return [null_count / count, ones_count / count]\n",
    "        \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        if depth == 0:\n",
    "            return\n",
    "        \n",
    "        is_leaf = False\n",
    "        \n",
    "        split_feature, split_value = self._find_best_split(X, y, self._criteria_func[self.criterion])\n",
    "        \n",
    "        if split_feature is None and split_value is None:\n",
    "            val = self._get_biggest_class(y)\n",
    "            return {'cond': val, 'leaf': True}\n",
    "        \n",
    "        left_inds = X[:, split_feature] < split_value - self.eps\n",
    "        right_inds = X[:, split_feature] >= split_value - self.eps\n",
    "\n",
    "        left_tree = self._build_tree(X[left_inds], y[left_inds], depth - 1)\n",
    "        right_tree = self._build_tree(X[right_inds], y[right_inds], depth - 1)\n",
    "        \n",
    "        if left_tree is None and right_tree is None:\n",
    "            is_leaf = True\n",
    "            \n",
    "        if is_leaf and split_feature is not None:\n",
    "            biggest_class = self._get_biggest_class(y)\n",
    "            proba = self._get_probs(y)\n",
    "            \n",
    "            return {'cond': biggest_class, 'leaf': True, 'proba': proba}\n",
    "            \n",
    "\n",
    "        return {'cond': (split_feature, split_value), 'leaf': is_leaf,\n",
    "                'left': left_tree, 'right': right_tree}\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.tree = self._build_tree(X, y, depth=self.max_depth)\n",
    "        return self        \n",
    "    \n",
    "    def _predict(self, X):\n",
    "        predictions = []\n",
    "        proba = []\n",
    "        for elem in X:\n",
    "            current_tree = self.tree\n",
    "            while type(current_tree['cond']) is tuple:\n",
    "                feature = current_tree['cond'][0]\n",
    "                value = current_tree['cond'][1]\n",
    "                if elem[feature] < value - self.eps:\n",
    "                    current_tree = current_tree['left']\n",
    "                else:\n",
    "                    current_tree = current_tree['right']\n",
    "            value = current_tree['cond']\n",
    "            if 'proba' in current_tree:\n",
    "                proba.append(current_tree['proba'])\n",
    "            elif value == 0:\n",
    "                proba.append([1.0, 0.0])\n",
    "            else:\n",
    "                proba.append([0.0, 1.0])\n",
    "            predictions.append(value)\n",
    "        return predictions, proba\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray):\n",
    "        _, proba = self._predict(X)\n",
    "        return proba\n",
    "    \n",
    "    def predict(self, X: np.ndarray): # получаем\n",
    "        predictions, _ = self._predict(X)\n",
    "        return predictions\n",
    "\n",
    "# X_clf = np.array([[-1, 1], [-1, -1], [2.5, 1], [1, 1], [2, 2], [1, -1]])\n",
    "# y_clf = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "X_clf = np.array([[1, 1], [2, -1], [-1, -1], [-1, -4], [2, 3], [3, 1]])\n",
    "y_clf = np.array([1, 1, 0, 0, 0, 0])\n",
    "\n",
    "model = MyDecisionTreeClassifier(max_depth=3, criterion='entropy').fit(X_clf, y_clf)\n",
    "print(model.tree)\n",
    "y_pred = model.predict(np.array([[2, 1], [0.5, 1]])) \n",
    "print(y_pred) # np.array([1, 0])\n",
    "y_prob = model.predict_proba(np.array([[2, 1], [0.5, 1]])) \n",
    "print(y_prob) #np.array([[0.0, 1.0], [1.0, 0.0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный Байес\n",
    "\n",
    "Требуется написать свой классификтор, на основе наивного баеса. Необходимо реализовать аналог MultinomialNB.\n",
    "\n",
    "$y_{test}=argmax_cln(P(y_{test}=c))+\\sum_{j=1}^mln(P(f_j|y_{test}=c)+ \\alpha)$, c∈{0,1}\n",
    "\n",
    "На вход подаются численные категориальные признаки. Классы: 00 и 11. У классификатора будет единственный параметр - alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 1: 2}\n",
      "{0: 0.5, 1: 0.5}\n",
      "{0: {0: {-1: 2}, 1: {-1: 1, 1: 1}}, 1: {0: {1: 2}, 1: {1: 1, -1: 1}}}\n",
      "[1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from collections import defaultdict\n",
    "from math import log, inf\n",
    "import numpy as np\n",
    "\n",
    "class MyNaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "        self.classes = [0, 1]\n",
    "        self.class_counts = {0: 0, 1: 0}\n",
    "        self.class_possibilities = {0: 0, 1: 0}\n",
    "        self.indicators = {0: {}, 1: {}}\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.class_counts = {0: 0, 1: 0}\n",
    "        self.class_possibilities = {0: 0, 1: 0}\n",
    "        self.indicators = {0: {}, 1: {}}\n",
    "        \n",
    "        n = y.shape[0]\n",
    "        features_len = len(X[0])\n",
    "        for cls in self.classes:\n",
    "            for j in range(features_len):\n",
    "                self.indicators[cls][j] = {}\n",
    "        \n",
    "        for i in range(n):\n",
    "            cls = y[i]\n",
    "            self.class_counts[cls] += 1\n",
    "            for feature_num in range(features_len):\n",
    "                feature_value = X[i][feature_num]\n",
    "                if feature_value not in self.indicators[cls][feature_num].keys():\n",
    "                    self.indicators[cls][feature_num][feature_value] = 0\n",
    "                self.indicators[cls][feature_num][feature_value] += 1\n",
    "    \n",
    "        for cls in self.classes:\n",
    "            self.class_possibilities[cls] = self.class_counts[cls] / n\n",
    "            \n",
    "        return self\n",
    "            \n",
    "    def predict(self, X: np.ndarray):\n",
    "        features_len = len(X[0])\n",
    "        result = []\n",
    "        for obj in X:\n",
    "            max_value = -inf\n",
    "            result_cls = None\n",
    "            for cls in self.classes:\n",
    "                value = log(self.class_possibilities[cls])\n",
    "                for feature_num in range(features_len):\n",
    "                    feature_value = obj[feature_num]\n",
    "                    if feature_value not in self.indicators[cls][feature_num].keys():\n",
    "                        value += log(self.alpha)\n",
    "                    else:\n",
    "                        value += log(self.indicators[cls][feature_num][feature_value] / self.class_counts[cls] + self.alpha)\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    result_cls = cls    \n",
    "            result.append(result_cls)\n",
    "        return result\n",
    "    \n",
    "X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_clf = np.array([1, 1, 0, 0])\n",
    "\n",
    "model = MyNaiveBayes(alpha=1).fit(X_clf, y_clf)\n",
    "\n",
    "print(model.class_counts)\n",
    "print(model.class_possibilities)\n",
    "print(model.indicators)\n",
    "\n",
    "y_pred = model.predict(np.array([[1, 2], [-1, -2]]))\n",
    "print(y_pred) # [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 8986 is different from 4312)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-b109cb719228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tweets_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tweets_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0mtest_sentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'airline_sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-b109cb719228>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(df_train, df_test)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mtest_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_frequencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;34m\"\"\"Calculate the posterior log probability of the samples X\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[1;32m    771\u001b[0m                 self.class_log_prior_)\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 8986 is different from 4312)"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "common_words = ['was', 'were', 'and', 'you', 'the', 'did']\n",
    "stops = set(stopwords.words(\"english\"))      \n",
    "\n",
    "def preprocess(df: pd.DataFrame):\n",
    "    wp = WordPunctTokenizer()\n",
    "    size = df.shape[0]\n",
    "    preprocessed = []\n",
    "    \n",
    "    for i in range(size):\n",
    "        sentence = df.iloc[i]['text']\n",
    "#         sentence_parts = sentence.split(' ')\n",
    "#         sentence_parts = list(filter(lambda x: '@' not in x and '#' not in x, sentence_parts))\n",
    "#         sentence = ' '.join(sentence_parts)\n",
    "        tokenized = wp.tokenize(sentence)\n",
    "        tokenized = list(filter(lambda x: \n",
    "                                len(x) > 2 and \n",
    "                                x not in common_words and\n",
    "                                re.search('\\d+', x) is None, tokenized))\n",
    "        new_sentence = ' '.join(tokenized)\n",
    "        preprocessed.append(new_sentence)\n",
    "    \n",
    "    return preprocessed\n",
    "\n",
    "def predict(df_train: pd.DataFrame, df_test: pd.DataFrame):\n",
    "    predictions = df_train[:]['airline_sentiment']\n",
    "    positive_train = df_train[df_train['airline_sentiment'] == 'positive']\n",
    "    negative_train = df_train[df_train['airline_sentiment'] == 'negative']\n",
    "    \n",
    "    positive_indices = positive_train.index.tolist()\n",
    "    negative_indices = negative_train.index.tolist()\n",
    "    \n",
    "    positive_len = len(positive_indices)\n",
    "    negative_len = len(negative_indices)\n",
    "    whole_len = positive_len + negative_len\n",
    "        \n",
    "    preprocessed_train = preprocess(df_train)    \n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    X = vectorizer.fit_transform(preprocessed_train)\n",
    "    frequencies = np.array(X.toarray())\n",
    "    \n",
    "#     preprocessed_test = preprocess(df_test)\n",
    "#     X_test = vectorizer.fit_transform(preprocessed_test)\n",
    "#     test_frequencies = np.array(X_test.toarray())\n",
    "    \n",
    "#     clf = MultinomialNB()\n",
    "#     clf.fit(frequencies, predictions)\n",
    "    \n",
    "#     test_predict = clf.predict(test_frequencies)\n",
    "#     print(test_predict)\n",
    "    \n",
    "    all_words = vectorizer.get_feature_names()\n",
    "    all_words_dict = dict((all_words[i], i) for i in range(len(all_words)))\n",
    "\n",
    "    positive_frequencies = frequencies[positive_indices]\n",
    "    negative_frequencies = frequencies[negative_indices]\n",
    "    \n",
    "    word_counts = frequencies.sum(axis=0)\n",
    "    positive_word_counts = positive_frequencies.sum(axis=0)\n",
    "    negative_word_counts = negative_frequencies.sum(axis=0)\n",
    "    \n",
    "    positive_word_frequencies = (positive_word_counts / word_counts) / positive_len * whole_len\n",
    "    negative_word_frequencies = (negative_word_counts / word_counts) / negative_len * whole_len\n",
    "    \n",
    "    preprocessed_test = preprocess(df_test)\n",
    "    predictions = []\n",
    "    \n",
    "    eps = 0.001\n",
    "    logged_eps = math.log(eps)\n",
    "    wp = WordPunctTokenizer()\n",
    "    \n",
    "    for i in range(len(preprocessed_test)):\n",
    "        sentence = preprocessed_test[i]\n",
    "        sentence_words = wp.tokenize(sentence)\n",
    "        positive_sum = 0\n",
    "        negative_sum = 0\n",
    "        for word in sentence_words:\n",
    "            if word in all_words_dict.keys():\n",
    "                word_index = all_words_dict[word]\n",
    "                positive_sum += math.log(positive_word_frequencies[word_index] + eps)\n",
    "                negative_sum += math.log(negative_word_frequencies[word_index] + eps)\n",
    "            else:\n",
    "                positive_sum += logged_eps\n",
    "                negative_sum += logged_eps\n",
    "        if positive_sum >= negative_sum:\n",
    "            predictions.append('positive')\n",
    "        else:\n",
    "            predictions.append('negative')   \n",
    "    return predictions\n",
    "\n",
    "train = pd.read_csv('./tweets_train.csv')\n",
    "test = pd.read_csv('./tweets_test.csv')\n",
    "predictions = predict(train, test)\n",
    "test_sentiments = test[:]['airline_sentiment'].tolist()\n",
    "true = 0\n",
    "i = 0\n",
    "for x, y in zip(predictions, test_sentiments):\n",
    "    if x == y: \n",
    "        true += 1\n",
    "#     else:\n",
    "#         print(i)\n",
    "#         print('true ', y)\n",
    "#         print(test.loc[i]['text'])\n",
    "    i += 1\n",
    "print(true / len(predictions))\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
