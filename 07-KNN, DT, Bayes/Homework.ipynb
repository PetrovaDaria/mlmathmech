{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Энтропия и критерий Джини\n",
    "\n",
    "$p_i$ - вероятность нахождения системы в i-ом состоянии.\n",
    "\n",
    "Энтропия Шеннона определяется для системы с N возможными состояниями следующим образом\n",
    "\n",
    "$S = - \\sum_{i=1}^Np_ilog_2p_i$\n",
    "\t \n",
    "Критерий Джини (Gini Impurity). Максимизацию этого критерия можно интерпретировать как максимизацию числа пар объектов одного класса, оказавшихся в одном поддереве.\n",
    "\n",
    "В общем случае критерий Джини считается как\n",
    "$G = 1 - \\sum_k(p_k)^2$\n",
    " \n",
    "Необходимо посчитать, значения Энтропии и критерия Джини"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.971, 0.48)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def get_possibilities(y):\n",
    "    count = len(y)\n",
    "    uniq_values = set(y)\n",
    "    possibilities = []\n",
    "    for value in uniq_values:\n",
    "        possibilities.append(len(y[y ==value]) / count)\n",
    "    return possibilities\n",
    "\n",
    "def gini_impurity(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * p\n",
    "    return round(1 - sum, 3)\n",
    "\n",
    "def entropy(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * math.log2(p)\n",
    "    return round(-sum, 3)\n",
    "\n",
    "def calc_criteria(y: np.ndarray) -> (float, float):\n",
    "    assert y.ndim == 1\n",
    "    return entropy(y), gini_impurity(y)\n",
    "\n",
    "y = np.array([1,1,1,1,1,1,0,0,0,0])\n",
    "calc_criteria(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information gain\n",
    "Вам надо реализовать функцию inform_gain, которая будет вычислять прирост информации для критерия (энтропия или критерий Джини) при разбиении выбрки по признаку (threshold).\n",
    "\n",
    "Прирост информации при разбиении выборки по признаку Q (например x≤12) определяется как\n",
    "\n",
    "$IG(Q)=S_0- \\sum_{i=1}^q\\frac{N_i}{N}S_i$\t\n",
    " \n",
    "где q - число групп после разбиения. $N_i$ - число элементов выборки, у которых признак Q имеет i-ое значение.\n",
    "\n",
    "И написать функцию get_best_threshold, которая будет находить наилучшее разбиение выборки.\n",
    "\n",
    "На вход подается:\n",
    "\n",
    "- X - одномерный массив - значения признака.\n",
    "- y - значения бинарных классов.\n",
    "- criteria_func - функция критерия, для которой вычислется наилучшее разбиение (Добавлять код из предыдущей задачи не нужно, мы сами передадим нужную функцию).\n",
    "- thr - значение разбиения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 1.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def get_possibilities(y):\n",
    "    count = len(y)\n",
    "    y = list(y)\n",
    "    uniq_values = set(y)\n",
    "    possibilities = []\n",
    "    for value in uniq_values:\n",
    "        possibilities.append(len(list(filter(lambda x: x == value, y))) / count)\n",
    "    return possibilities\n",
    "\n",
    "def gini_impurity(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * p\n",
    "    return round(1 - sum, 3)\n",
    "\n",
    "def entropy(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * math.log2(p)\n",
    "    return round(-sum, 3)\n",
    "\n",
    "def len_check_criteria_func(arr, criteria_func):\n",
    "    if len(arr) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return criteria_func(arr)\n",
    "\n",
    "def inform_gain(X: np.ndarray, y: np.ndarray, threshold: float, criteria_func) -> float:\n",
    "    s0 = criteria_func(y)\n",
    "    count = y.shape[0]\n",
    "    first = []\n",
    "    second = []\n",
    "    for i in range(count):\n",
    "        if X[i] <= threshold:\n",
    "            first.append(y[i])\n",
    "        else:\n",
    "            second.append(y[i])\n",
    "    s1 = len_check_criteria_func(first, criteria_func)\n",
    "    s2 = len_check_criteria_func(second, criteria_func)\n",
    "    return s0 - len(first) / count * s1 - len(second) / count * s2\n",
    "            \n",
    "\n",
    "def get_best_threshold(X: np.ndarray, y: np.ndarray, criteria_func) -> (float, float):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    uniq_values = set(X)\n",
    "    for value in uniq_values:\n",
    "        score = inform_gain(X, y, value, criteria_func)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = value\n",
    "    return best_threshold, best_score\n",
    "\n",
    "X = np.array([3, 9, 0, 4, 7, 2, 1, 6, 8, 5])\n",
    "y = np.array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1])\n",
    "threshold=3\n",
    "criteria_func=entropy\n",
    "print(inform_gain(X, y, threshold, criteria_func))\n",
    "\n",
    "X = np.array([3, 9, 0, 4, 7, 2, 1, 6, 8, 5])\n",
    "y = np.array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1])\n",
    "criteria_func=entropy\n",
    "get_best_threshold(X, y, criteria_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6099865470109875\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(1 -(-(5/6) * math.log2(5/6) - (1/6)* math.log2(1/6))* 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best split\n",
    "\n",
    "Реализуйте функцию find_best_split, которая находит наилучшее разбиение по всем признакам. На вход подется обучающая выборка и функция критерий. Необходимо вернуть: индекс фичи, значение границы (threshold) и результат разбиение (information gain).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -1, 1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_possibilities(y):\n",
    "    count = len(y)\n",
    "    y = list(y)\n",
    "    uniq_values = set(y)\n",
    "    possibilities = []\n",
    "    for value in uniq_values:\n",
    "        possibilities.append(len(list(filter(lambda x: x == value, y))) / count)\n",
    "    return possibilities\n",
    "\n",
    "def gini_impurity(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * p\n",
    "    return round(1 - sum, 3)\n",
    "\n",
    "def entropy(y: np.ndarray) -> float:\n",
    "    possibilities = get_possibilities(y)\n",
    "    sum = 0\n",
    "    for p in possibilities:\n",
    "        sum += p * math.log2(p)\n",
    "    return round(-sum, 3)\n",
    "\n",
    "def inform_gain(X: np.ndarray, y: np.ndarray, threshold: float, criteria_func) -> float:\n",
    "    s0 = criteria_func(y)\n",
    "    count = y.shape[0]\n",
    "    first = []\n",
    "    second = []\n",
    "    for i in range(count):\n",
    "        if X[i] <= threshold:\n",
    "            first.append(y[i])\n",
    "        else:\n",
    "            second.append(y[i])\n",
    "    s1 = criteria_func(first)\n",
    "    s2 = criteria_func(second)\n",
    "    return s0 - len(first) / count * s1 - len(second) / count * s2\n",
    "            \n",
    "\n",
    "def get_best_threshold(X: np.ndarray, y: np.ndarray, criteria_func) -> (float, float):\n",
    "    assert X.ndim == 1\n",
    "    assert y.ndim == 1\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    uniq_values = set(X)\n",
    "    for value in uniq_values:\n",
    "        score = inform_gain(X, y, value, criteria_func)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = value\n",
    "    return best_threshold, best_score\n",
    "\n",
    "def find_best_split(X, y, criteria_func):\n",
    "    assert X.ndim == 2\n",
    "    assert y.ndim == 1\n",
    "    best_feature = 0\n",
    "    best_score = 0\n",
    "    best_threshold = 0\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        feature_column = X[:, i]\n",
    "        threshold, score = get_best_threshold(feature_column, y, criteria_func)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_feature = i\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_feature, best_threshold, best_score\n",
    "\n",
    "X = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y = np.array([1, 1, 0, 0])\n",
    "criteria_func=entropy\n",
    "find_best_split(X, y, criteria_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мое дерево решений\n",
    "\n",
    "Ваша задача реализовать свой простой KNNClassifier для бинарных данных. Вам нужно реализовать 3 метода:\n",
    "\n",
    "fit - обучение классификатора\n",
    "predict - предсказание для новых объектов\n",
    "predict_proba - предсказание вероятностей новых объектов\n",
    "У нашего классификатора будет лишь два гиперпараметра - максимальная глубина дерева max_depth и критерий разбиения criterion. Энтропия или Джини.\n",
    "\n",
    "Все функции из предыдущих заданий нужно добавить в этот код.\n",
    "\n",
    "На вход будет подаваться выборка объектов X. y - результат бинарной классификации 0 или 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val': (0, -1), 'leaf': False, 'left': {'val': (0, 0), 'leaf': False, 'left': None, 'right': None}, 'right': {'val': (0, 0), 'leaf': False, 'left': None, 'right': None}}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MyDecisionTreeClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=4, criterion='entropy'): \n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion # 'entropy' or 'gini' \n",
    "        self.tree = {}\n",
    "        self._criteria_func = {\n",
    "            'gini': self._gini_impurity,\n",
    "            'entropy': self._entropy\n",
    "        }\n",
    "        \n",
    "    def _get_possibilities(self, y):\n",
    "        count = len(y)\n",
    "        y = list(y)\n",
    "        uniq_values = set(y)\n",
    "        possibilities = []\n",
    "        for value in uniq_values:\n",
    "            possibilities.append(len(list(filter(lambda x: x == value, y))) / count)\n",
    "        return possibilities\n",
    "\n",
    "    def _entropy(self, y: np.ndarray) -> float:\n",
    "        possibilities = self._get_possibilities(y)\n",
    "        sum = 0\n",
    "        for p in possibilities:\n",
    "            sum += p * math.log2(p)\n",
    "        return round(-sum, 3)\n",
    "\n",
    "    def _gini_impurity(self, y: np.ndarray) -> float:\n",
    "        possibilities = self._get_possibilities(y)\n",
    "        sum = 0\n",
    "        for p in possibilities:\n",
    "            sum += p * p\n",
    "        return round(1 - sum, 3)\n",
    "        \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        if depth == 0:\n",
    "            return\n",
    "        split_feature, split_value = self._find_best_split(X, y, self._criteria_func[self.criterion])\n",
    "\n",
    "        left_inds = X[:, split_feature] <= split_value\n",
    "        right_inds = X[:, split_feature] > split_value\n",
    "\n",
    "        left_tree = self._build_tree(X[left_inds], y[left_inds], depth - 1)\n",
    "        right_tree = self._build_tree(X[right_inds], y[right_inds], depth - 1)\n",
    "\n",
    "        return {'val': (split_feature, split_value), 'leaf': False,\n",
    "                'left': left_tree, 'right': right_tree}\n",
    "    \n",
    "    def _find_best_split(self, X, y, criteria_func):\n",
    "        best_feature = 0\n",
    "        best_score = 0\n",
    "        best_threshold = 0\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            feature_column = X[:, i]\n",
    "            threshold, score = self._get_best_threshold(feature_column, y, criteria_func)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_feature = i\n",
    "                best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _get_best_threshold(self, X: np.ndarray, y: np.ndarray, criteria_func) -> (float, float):\n",
    "        best_threshold = 0\n",
    "        best_score = 0\n",
    "        uniq_values = set(X)\n",
    "        for value in uniq_values:\n",
    "            score = self._inform_gain(X, y, value, criteria_func)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = value\n",
    "        return best_threshold, best_score\n",
    "    \n",
    "    def _inform_gain(self, X: np.ndarray, y: np.ndarray, threshold: float, criteria_func) -> float:\n",
    "        s0 = criteria_func(y)\n",
    "        count = y.shape[0]\n",
    "        first = []\n",
    "        second = []\n",
    "        for i in range(count):\n",
    "            if X[i] <= threshold:\n",
    "                first.append(y[i])\n",
    "            else:\n",
    "                second.append(y[i])\n",
    "        s1 = criteria_func(first)\n",
    "        s2 = criteria_func(second)\n",
    "        return s0 - len(first) / count * s1 - len(second) / count * s2\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.tree = self._build_tree(X, y, depth=self.max_depth)\n",
    "        return self        \n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X: np.ndarray): # получаем \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "    \n",
    "X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_clf = np.array([1, 1, 0, 0])\n",
    "\n",
    "model = MyDecisionTreeClassifier(max_depth=2, criterion='entropy').fit(X_clf, y_clf)\n",
    "print(model.tree)\n",
    "y_pred = model.predict(np.array([[2, 2], [-2, -2]])) # np.array([1, 0])\n",
    "y_prob = model.predict_proba(np.array([[2, 2], [-2, -2]])) #np.array([[0.0, 1.0], [1.0, 0.0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный Байес\n",
    "\n",
    "Требуется написать свой классификтор, на основе наивного баеса. Необходимо реализовать аналог MultinomialNB.\n",
    "\n",
    "$y_{test}=argmax_cln(P(y_{test}=c))+\\sum_{j=1}^mln(P(f_j|y_{test}=c)+ \\alpha)$, c∈{0,1}\n",
    "\n",
    "На вход подаются численные категориальные признаки. Классы: 00 и 11. У классификатора будет единственный параметр - alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 1: 2}\n",
      "{0: 0.5, 1: 0.5}\n",
      "{0: {0: {-1: 2}, 1: {-1: 1, 1: 1}}, 1: {0: {1: 2}, 1: {1: 1, -1: 1}}}\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from collections import defaultdict\n",
    "from math import log, inf\n",
    "import numpy as np\n",
    "\n",
    "class MyNaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "        self.classes = [0, 1]\n",
    "        self.class_counts = {0: 0, 1: 0}\n",
    "        self.class_possibilities = {0: 0, 1: 0}\n",
    "        self.indicators = {0: {}, 1: {}}\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.class_counts = {0: 0, 1: 0}\n",
    "        self.class_possibilities = {0: 0, 1: 0}\n",
    "        self.indicators = {0: {}, 1: {}}\n",
    "        \n",
    "        n = y.shape[0]\n",
    "        features_len = len(X[0])\n",
    "        for cls in self.classes:\n",
    "            for j in range(features_len):\n",
    "                self.indicators[cls][j] = {}\n",
    "        \n",
    "        for i in range(n):\n",
    "            cls = y[i]\n",
    "            self.class_counts[cls] += 1\n",
    "            for feature_num in range(features_len):\n",
    "                feature_value = X[i][feature_num]\n",
    "                if feature_value not in self.indicators[cls][feature_num].keys():\n",
    "                    self.indicators[cls][feature_num][feature_value] = 0\n",
    "                self.indicators[cls][feature_num][feature_value] += 1\n",
    "    \n",
    "        for cls in self.classes:\n",
    "            self.class_possibilities[cls] = self.class_counts[cls] / n\n",
    "            \n",
    "        return self\n",
    "            \n",
    "    def predict(self, X: np.ndarray):\n",
    "        features_len = len(X[0])\n",
    "        result = []\n",
    "        for obj in X:\n",
    "            max_value = -inf\n",
    "            result_cls = None\n",
    "            for cls in self.classes:\n",
    "                value = log(self.class_possibilities[cls])\n",
    "                for feature_num in range(features_len):\n",
    "                    feature_value = obj[feature_num]\n",
    "                    if feature_value not in self.indicators[cls][feature_num].keys():\n",
    "                        value += log(self.alpha)\n",
    "                    else:\n",
    "                        value += log(self.indicators[cls][feature_num][feature_value] / self.class_counts[cls] + self.alpha)\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    result_cls = cls    \n",
    "            result.append(result_cls)\n",
    "        return result\n",
    "    \n",
    "X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_clf = np.array([1, 1, 0, 0])\n",
    "\n",
    "model = MyNaiveBayes(alpha=1).fit(X_clf, y_clf)\n",
    "\n",
    "print(model.class_counts)\n",
    "print(model.class_possibilities)\n",
    "print(model.indicators)\n",
    "\n",
    "y_pred = model.predict(np.array([[1, 2], [-1, -2]]))\n",
    "print(y_pred) # [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
