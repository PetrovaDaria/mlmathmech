{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\n",
    "from pandas.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Основы метрик классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подаются 2 массива $y_{real}$ - реальные значения бинарных классов и $y_{pred}$ - предсказанные значения бинарных классов. Вам необходимо посчитать, не используя стандартные функции, метрики: \n",
    "* $accuracy$\n",
    "* $precision$\n",
    "* $recall$\n",
    "* $F_1$\n",
    "\n",
    "Возвращать числа нужно именно в данном порядке.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "y_real = np.array([0, 1, 0, 0, 1, 1, 1, 1])\n",
    "y_pred = np.array([0, 1, 1, 0, 1, 1, 0, 0])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "0.625, 0.75, 0.6, 0.66\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_metrics(y_real, y_pred) -> (float, float, float, float):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_metrics(y_real, y_pred) -> (float, float, float, float):\n",
    "    tp = (y_real * y_pred).sum()\n",
    "    fp = y_pred.sum() - tp\n",
    "    tn = ((1 - y_real) * (1 - y_pred)).sum()\n",
    "    fn = (1 - y_pred).sum() - tn\n",
    "    \n",
    "    accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return accuracy, precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "######################################################\n",
    "y_real = np.array([0, 1, 0, 0, 1, 1, 1, 1])\n",
    "y_pred = np.array([0, 1, 1, 0, 1, 1, 0, 0])\n",
    "\n",
    "\n",
    "acc, pre, rec, f1 = main_metrics(y_real, y_pred)\n",
    "\n",
    "assert np.abs(acc - accuracy_score(y_real, y_pred)) < 0.001\n",
    "assert np.abs(pre - precision_score(y_real, y_pred)) < 0.001\n",
    "assert np.abs(rec - recall_score(y_real, y_pred)) < 0.001\n",
    "assert np.abs(f1  - f1_score(y_real, y_pred)) < 0.001\n",
    "######################################################\n",
    "y_real = np.random.choice(2, 1000)\n",
    "y_pred = np.random.choice(2, 1000)\n",
    "\n",
    "acc, pre, rec, f1 = main_metrics(y_real, y_pred)\n",
    "\n",
    "assert np.abs(acc - accuracy_score(y_real, y_pred)) < 0.001\n",
    "assert np.abs(pre - precision_score(y_real, y_pred)) < 0.001\n",
    "assert np.abs(rec - recall_score(y_real, y_pred)) < 0.001\n",
    "assert np.abs(f1  - f1_score(y_real, y_pred)) < 0.001\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Основы метрик регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решаем задачу регрессии. На вход подаются 2 массива $y_{real}$ - реальные значения функции и $y_{pred}$ - предсказанные значения функции. Вам необходимо посчитать, не используя стандартные функции, метрики: \n",
    "* $R^2score$\n",
    "* $MAE$ - `mean_absolute_error`\n",
    "* $MSE$ - `mean_squared_error`\n",
    "* $MSLE$ - `mean_squared_log_error`\n",
    "\n",
    "Возвращать числа нужно именно в данном порядке.\n",
    "\n",
    "Формулы для метрик - ищите в интернете, это часть задания. Можете сверяться с реальными метриками в `sklearn.metrics`.\n",
    "\n",
    "Все числа в тестах больше 0, поэтому $MSLE$ будет считаться корректно\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "y_real = np.array([1, 2, 3, 4, 6])\n",
    "y_pred = np.array([1, 3, 2, 4, 5])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "0.797297, 0.6, 0.6, 0.037856\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_metrics(y_real, y_pred) -> (float, float, float, float):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_metrics(y_real, y_pred) -> (float, float, float, float):\n",
    "    R2 = 1 - np.sum((y_real - y_pred)**2) / np.sum((y_real - y_real.mean())**2)\n",
    "    MAE = np.mean(np.abs(y_real- y_pred))\n",
    "    MSE = np.mean((y_real- y_pred)**2)\n",
    "    MSLE = np.mean((np.log(y_real + 1) - np.log(y_pred + 1))**2)\n",
    "    return R2, MAE, MSE, MSLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as R2, mean_absolute_error as MAE, mean_squared_log_error as MSLE, mean_squared_error as MSE \n",
    "######################################################\n",
    "y_real = np.array([1,2,3,4,6])\n",
    "y_pred = np.array([1,3,2,4,5])\n",
    "\n",
    "\n",
    "r2, mae, mse, msle = reg_metrics(y_real, y_pred)\n",
    "\n",
    "assert np.abs(r2 - R2(y_real, y_pred)) < 0.001\n",
    "assert np.abs(mae - MAE(y_real, y_pred)) < 0.001\n",
    "assert np.abs(mse - MSE(y_real, y_pred)) < 0.001\n",
    "assert np.abs(msle  - MSLE(y_real, y_pred)) < 0.001\n",
    "######################################################\n",
    "y_real = np.random.choice(1000, 1000)\n",
    "y_pred = np.random.choice(1000, 1000)\n",
    "\n",
    "r2, mae, mse, msle = reg_metrics(y_real, y_pred)\n",
    "\n",
    "assert np.abs(r2 - R2(y_real, y_pred)) < 0.001\n",
    "assert np.abs(mae - MAE(y_real, y_pred)) < 0.001\n",
    "assert np.abs(mse - MSE(y_real, y_pred)) < 0.001\n",
    "assert np.abs(msle  - MSLE(y_real, y_pred)) < 0.001\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) LogLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте свой LogLoss метрику.\n",
    "\n",
    "В результат $y_{real}$ - передаются значения бинарных классов. В $y_{prob}$ - передаются вероятности $P(y_{pred}=1)$. Чтобы обработать краевые случаи в $y_{prob}$: \n",
    "* $0$ нужно заменить на `eps`\n",
    "* $1$ нужно заменить на ($1$ - `eps`).\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "y_real = np.array([  1,   1,   0,   0,   1,   0,   1,   0])\n",
    "y_prob = np.array([  1, 0.8, 0.2, 0.2, 0.4, 0.4, 0.6,   0])\n",
    "```\n",
    "#### Output:\n",
    "``` python\n",
    "    0.325921\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_real, y_pred, eps=1e-15):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_real, y_prob, eps=1e-15):\n",
    "    np.place(y_prob, np.abs(y_prob - 0.) < eps, eps)\n",
    "    np.place(y_prob, np.abs(y_prob - 1.) < eps, 1 - eps)\n",
    "    return - np.mean(y_real*np.log(y_prob) + (1 - y_real)*np.log(1 - y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "######################################################\n",
    "y_real = np.array([  1,   1,   0,   0,   1,   0,   1,   0])\n",
    "y_prob = np.array([  1, 0.8, 0.2, 0.2, 0.4, 0.4, 0.6,   0])\n",
    "\n",
    "assert logloss(y_real, y_prob) - sklearn.metrics.log_loss(y_real, y_prob) < 0.001 \n",
    "######################################################\n",
    "y_real = np.random.choice(2, 1000)\n",
    "y_prob = np.random.choice(1000, 1000) / 1000\n",
    "\n",
    "assert logloss(y_real, y_prob) - sklearn.metrics.log_loss(y_real, y_prob) < 0.001 \n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) Нахождение Roc-curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам на вход даны $y_{real}$ и массив вероятностей $P(y_{pred}=1)$ необходимо реализовать функцию `roc-curve`, которая вернет 2 массива различных значений fpr и tpr, для дальнейшего построения Roc кривой.\n",
    "\n",
    "Можно считать, что все вероятности ограничены $decimal=2$ (у каждого числа не более 2-х знаков после запятой)\n",
    "\n",
    "### Sample\n",
    "#### Input:\n",
    "```python\n",
    "y_real = np.array([  1,   1,   0,   0,   0,   1,   0,   1,   0])\n",
    "y_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.6, 0.4, 0.6, 0.6, 0.4])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([0.,  0.,  0.4, 0.6, 1. ]), #fpr\n",
    "array([0., 0.5, 0.75,  1., 1. ])  #tpr\n",
    "```\n",
    "\n",
    "### Sample 2\n",
    "#### Input:\n",
    "```python\n",
    "y_real = np.array([  1,   1,   0,   0,   1,   0,   1,   0])\n",
    "y_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.4, 0.4, 0.6, 0.6])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([0.,  0., 0.25, 0.5, 1. ]), #fpr\n",
    "array([0., 0.5, 0.75,  1., 1. ])  #tpr\n",
    "\n",
    "или \n",
    "\n",
    "array([0.,  0., 0.5, 1. ]), #fpr\n",
    "array([0., 0.5,  1., 1. ])  #tpr\n",
    "```\n",
    "\n",
    "Обратите внимание на 2 пример: roc кривая, которая задается ими - одинаковая. Точка, которая уходит, находится на прямой между двумя соседними, в целом такие точки можно убирать, но будут приниматься оба варианта. Функция `sklearn.metrics.roc_curve` возвращает второй вариант."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(y_real, y_prob) -> (np.array, np.array):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(y_real, y_prob):\n",
    "    fpr=[]\n",
    "    tpr=[]\n",
    "    yprob1 = y_prob[y_real==1] \n",
    "    yprob0 = y_prob[y_real==0]\n",
    "    for border in np.arange(1.01, -0.01, -0.01):\n",
    "        tp = (yprob1 >= border).sum()\n",
    "        fp = (yprob0 >= border).sum()\n",
    "        tn = (yprob0 < border).sum()\n",
    "        fn = (yprob1 < border).sum()\n",
    "        _tpr = tp / (tp + fn)\n",
    "        _fpr = fp / (tn + fp)\n",
    "        if len(fpr) == 0 or np.abs(fpr[-1] - _fpr) > 0.001 or np.abs(tpr[-1] - _tpr) > 0.001:\n",
    "            fpr.append(_fpr)\n",
    "            tpr.append(_tpr)\n",
    "    \n",
    "    return np.array(fpr), np.array(tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "######################################################\n",
    "y_real = np.array([  1,   1,   0,   0,   0,   1,   0,   1,   0])\n",
    "y_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.6, 0.4, 0.6, 0.6, 0.4])\n",
    "fpr_true, tpr_true, _ = roc_curve(y_real, y_prob)\n",
    "fpr, tpr = roc(y_real, y_prob)\n",
    "\n",
    "assert auc(fpr, tpr) - auc(fpr_true, tpr_true) < 0.01\n",
    "######################################################\n",
    "y_real = np.array([  1,   1,   0,   0,   1,   0,   1,   0])\n",
    "y_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.4, 0.4, 0.6, 0.6])\n",
    "fpr_true, tpr_true, _  = roc_curve(y_real, y_prob)\n",
    "fpr, tpr = roc(y_real, y_prob)\n",
    "\n",
    "assert auc(fpr, tpr) - auc(fpr_true, tpr_true) < 0.01\n",
    "######################################################\n",
    "y_real = np.random.choice(2, 1000) \n",
    "y_prob = np.random.choice(101, 1000) / 100\n",
    "\n",
    "fpr_true, tpr_true, _  = roc_curve(y_real, y_prob)\n",
    "fpr, tpr = roc(y_real, y_prob)\n",
    "\n",
    "assert auc(fpr, tpr) - auc(fpr_true, tpr_true) < 0.01\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Кросс-валидация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход падаются [данные](https://yadi.sk/d/6pUM_Ko-RtqiZg) и $k$ - число фолдов в кросс-валидации. Верните значения кросс-валидации по $k$ фолдам алгоритма `LogisticRegression` с метрикой `neg_log_loss`. \n",
    "\n",
    "В итоге должны получиться значения не ниже $-1$.\n",
    "\n",
    "### Sample\n",
    "#### Input:\n",
    "```python\n",
    "X = pd.read_csv('resources/train.csv').drop(columns=['y']).values\n",
    "Y = pd.read_csv('resources/train.csv')['y']\n",
    "k = 3\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([-0.45456358, -0.41684932, -0.49260998])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate(X, y, k):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "def crossvalidate(X, y, k):\n",
    "    return cross_val_score(LogisticRegression(), X, y, cv = k, scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('resources/train.csv').drop(columns=['y']).values\n",
    "Y = pd.read_csv('resources/train.csv')['y']\n",
    "######################################################\n",
    "assert crossvalidate(X, Y, 3).min() > -1\n",
    "assert crossvalidate(X, Y, 4).min() > -1\n",
    "assert crossvalidate(X, Y, 5).min() > -1\n",
    "assert crossvalidate(X, Y, 6).min() > -1\n",
    "assert crossvalidate(X, Y, 7).min() > -1\n",
    "assert crossvalidate(X, Y, 8).min() > -1\n",
    "assert crossvalidate(X, Y, 9).min() > -1\n",
    "\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45456358, -0.41684932, -0.49260998])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossvalidate(X, Y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C помощью [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) найдите лучшие коэффициенты гиперпараметров `max_depth` и `min_samples_leaf` для классификатора [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) и верните обученный grid_search. \n",
    "\n",
    "* Пределы `max_depth` $(1, 10)$ \n",
    "* Пределы `min_samples_leaf` $(1, 10)$  \n",
    "* Входные данные по [ссылке](https://yadi.sk/d/6pUM_Ko-RtqiZg)\n",
    "* scoring - `precision`\n",
    "* cv - $5$\n",
    "* Другие параметры в `DecisionTreeClassifier` не указывать.\n",
    "\n",
    "Не нужно Shuffl-ить данные, это может повлиять на ответ и в итоге задача не зачтется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def fit_gs(X: np.ndarray, y: np.ndarray) ->  GridSearchCV:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def fit_gs(X: np.ndarray, y:np.ndarray) ->  GridSearchCV:\n",
    "    params = {'max_depth': np.arange(1, 10), \n",
    "              'min_samples_leaf': np.arange(1, 10)}\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "            estimator = DTC(), #классификатор\n",
    "            param_grid = params, # перебираемый параметр\n",
    "            scoring='precision',\n",
    "            cv = 5\n",
    "         )\n",
    "    gs.fit(X, y)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('resources/train.csv').drop(columns=['y']).values\n",
    "Y = pd.read_csv('resources/train.csv')['y']\n",
    "######################################################\n",
    "gs = fit_gs(X,Y)\n",
    "\n",
    "assert gs.best_params_['max_depth'] == 4\n",
    "assert gs.best_params_['min_samples_leaf'] == 3\n",
    "assert np.abs(gs.best_score_ - 0.7829244) < 0.001\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C помощью [RandomSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) найдите лучшие коэффициенты гиперпараметров `max_depth` и `min_samples_leaf` для классификатора [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). \n",
    "\n",
    "Обучите $2$ `DTC`. Один с количеством итераций - $10$, другой с количеством итераций $50$. Верните обе обученные модели в соответствующем порядке.\n",
    "\n",
    "* Пределы `max_depth` $(1, 10)$ \n",
    "* Пределы `min_samples_leaf` $(1, 10)$  \n",
    "* Входные данные по [ссылке](https://yadi.sk/d/6pUM_Ko-RtqiZg)\n",
    "* scoring - `precision`\n",
    "* cv - $5$\n",
    "* Другие параметры в `DecisionTreeClassifier` не указывать.\n",
    "\n",
    "Не нужно Shuffl-ить данные, это может повлиять на ответ и в итоге задача не зачтется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import  RandomizedSearchCV\n",
    "\n",
    "def fit_gs(X: np.ndarray, y: np.ndarray) -> (RandomizedSearchCV, RandomizedSearchCV):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def fit_gs(X: np.ndarray, y:np.ndarray) ->  (RandomizedSearchCV, RandomizedSearchCV):\n",
    "    params = {'max_depth': np.arange(1, 10), \n",
    "              'min_samples_leaf': np.arange(1, 10)}\n",
    "\n",
    "    rs1 = RandomizedSearchCV(\n",
    "            n_iter = 10,\n",
    "            estimator = DTC(), #классификатор\n",
    "            param_distributions= params, # перебираемый параметр\n",
    "            scoring='precision',\n",
    "            cv = 5,\n",
    "            iid=None\n",
    "         )\n",
    "    rs1.fit(X, y)\n",
    "    \n",
    "    rs2 = RandomizedSearchCV(\n",
    "            n_iter = 50,\n",
    "            estimator = DTC(), #классификатор\n",
    "            param_distributions= params, # перебираемый параметр\n",
    "            scoring='precision',\n",
    "            cv = 5,\n",
    "            iid=None\n",
    "         )\n",
    "    rs2.fit(X, y)\n",
    "    \n",
    "    return rs1, rs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('resources/train.csv').drop(columns=['y']).values\n",
    "Y = pd.read_csv('resources/train.csv')['y']\n",
    "######################################################\n",
    "rs1, rs2 = fit_gs(X, Y)\n",
    "\n",
    "assert rs1.best_params_['min_samples_leaf'] > 2\n",
    "assert rs2.best_params_['min_samples_leaf'] > 2\n",
    "assert rs2.best_params_['max_depth'] == 4\n",
    "\n",
    "assert rs1.best_score_ > 0.75\n",
    "assert rs2.best_score_ > 0.78\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) Моя Кросс-валидация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам необходимо реальзовать собственную функцию кросс-валидации. \n",
    "\n",
    "Напоминание, кросс-валидация: \n",
    "1. Разбивает, предварительно перемешав, данные на $k$ (заданное) `фолдов` данных (если нацело данные не делятся - хвост отбрасываем).\n",
    "2. Берет один из фолдов за тестовую выборку, а все остальное за тренировачную.\n",
    "3. Обучает эстиматор на тренировачных данныx \n",
    "4. Cчитает `score` обученной модели.\n",
    "5. 2-4 шаги повторяются для всех $k$ фолдов \n",
    "\n",
    "Она должна принимать на вход: \n",
    "* `estimator`, который мы хотим обучить на разных фолдах данных\n",
    "* $X, y$ - данные \n",
    "* `cv` ($k$) - объект разбиения `KFold` или количество фолдов, на которые мы бьем данные (по умолчанию 3)\n",
    "* `scoring` - функция метрики, по которой оцениваем полученные результаты (замечание: в реальной `cross_val_score` здесь будет стоять объект `scorer`, у нас же будет стоять функция, например `metrics.accuracy_score`)\n",
    "\n",
    "На выходе мы получаем массив длины $k$ из score значений.\n",
    "\n",
    "Подсказка: используйте `model_selection.KFold` для разбиения выборки.\n",
    "\n",
    "Подсказка: не нужно shuffl-ить данные при создании `KFold` из `cv`! \n",
    "\n",
    "### Sample\n",
    "#### Input:\n",
    "```python\n",
    "estimator = LinearRegression()\n",
    "X = np.array([[1],[2],[3],[4],[5]])\n",
    "y = np.array([1, 2, 4, 4, 6])\n",
    "cv = 2\n",
    "scoring = sklearn.metrics.r2_score\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([-2.64285714 -0.23611111])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "def my_cross_val_score(estimator, X, y, cv=3, scoring=accuracy_score, random_state=42):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "def my_cross_val_score(estimator, X, y, cv=3, scoring=accuracy_score):\n",
    "    if isinstance(cv, int):\n",
    "        kf = KFold(n_splits=cv)\n",
    "    else:\n",
    "        kf = cv\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf = estimator.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        scores.append(scoring(y_test, y_pred))    \n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.64285714 -0.23611111]\n",
      "[-2.64285714 -0.23611111]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "######################################################\n",
    "np.random.seed(228)\n",
    "n = 400\n",
    "a = np.random.normal(loc=0, scale=1, size=(n, 2)) #первый класс\n",
    "b = np.random.normal(loc=3, scale=2, size=(n, 2)) #второй класс\n",
    "X = np.vstack([a, b]) #двумерный количественный признак\n",
    "y = np.hstack([np.zeros(n), np.ones(n)]) #бинарный признак\n",
    "\n",
    "cvs = model_selection.cross_val_score(RandomForestClassifier(), X, y, cv=5, scoring='accuracy')\n",
    "cvs_my = my_cross_val_score(RandomForestClassifier(), X, y, cv=5, scoring=accuracy_score)\n",
    "\n",
    "assert (np.abs(cvs - cvs_my) < 0.1).all()\n",
    "######################################################\n",
    "estimator = LinearRegression()\n",
    "X = np.array([[1],[2],[3],[4],[5]])\n",
    "y = np.array([1, 2, 4, 4, 6])\n",
    "\n",
    "cvs = model_selection.cross_val_score(estimator, X, y, cv=2, scoring='r2')\n",
    "cvs_my = my_cross_val_score(estimator, X, y, cv=2, scoring=r2_score)\n",
    "print(cvs)\n",
    "print(cvs_my)\n",
    "assert (np.abs(cvs - cvs_my) < 0.01).all()\n",
    "\n",
    "######################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
