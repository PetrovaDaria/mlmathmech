{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод ближайших соседей\n",
    "\n",
    "Метод ближайших соседей (k Nearest Neighbors, или kNN) — популярный метод классификации, также иногда используемый в задачах регрессии. Это один из самых понятных подходов к классификации. На уровне интуиции суть метода такова: посмотри на соседей, какие преобладают, таков и ты. Если метрика расстояния между примерами введена достаточно удачно, то схожие примеры гораздо чаще лежат в одном классе, чем в разных. \n",
    "\n",
    "Согласно методу ближайших соседей, тестовый пример (зеленый шарик) будет отнесен к классу \"синие\", а не \"красные\".\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/808/0*TMLaS4woI6xo6RKs.png\">\n",
    "\n",
    "Например, если не знаешь, какой тип товара указать в объявлении для Bluetooth-гарнитуры, можешь найти 5 похожих гарнитур, и если 4 из них отнесены к категории \"Аксессуары\", и только один - к категории \"Техника\", то здравый смысл подскажет для своего объявления тоже указать категорию \"Аксессуары\".\n",
    "\n",
    "Для классификации каждого из объектов тестовой выборки необходимо последовательно выполнить следующие операции:\n",
    " - Вычислить расстояние до каждого из объектов обучающей выборки\n",
    " - Отобрать $k$ объектов обучающей выборки, расстояние до которых минимально\n",
    " - Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди $k$ ближайших соседей\n",
    " \n",
    " Примечательное свойство такого подхода  – его ленивость. Это значит, что вычисления начинаются только в момент классификации тестового примера, а заранее, только при  наличии обучающих примеров, никакая модель не строится.\n",
    " \n",
    "Стоит отметить, что метод ближайших соседей – хорошо изученный подход (в машинном обучении, эконометрике и статистике больше известно наверно только про линейную регрессию). Для метода ближайших соседей существует немало важных теорем, утверждающих, что на \"бесконечных\" выборках это оптимальный метод классификации. Авторы классической книги \"The Elements of Statistical Learning\" считают kNN теоретически идеальным алгоритмом, применимость которого просто ограничена вычислительными возможностями и проклятием размерностей. \n",
    "\n",
    "### Метод ближайших соседей в реальных задачах\n",
    "- В чистом виде kNN может послужить хорошим стартом (baseline) в решении какой-либо задачи;\n",
    "- В соревнованиях Kaggle kNN часто используется для построения мета-признаков (прогноз kNN подается на вход прочим моделям) или в стекинге/блендинге;\n",
    "- Идея ближайшего соседа расширяется и на другие задачи, например, в рекомендательных системах простым начальным решением может быть рекомендация какого-то товара (или услуги), популярного среди *ближайших соседей* человека, которому хотим сделать рекомендацию;\n",
    "\n",
    "### Гиперпараметры\n",
    "Качество классификации методом ближайших соседей зависит от нескольких параметров:\n",
    " - число соседей\n",
    " - метрика расстояния между объектами (часто используются метрика Хэмминга, евклидово расстояние, косинусное расстояние и расстояние Минковского). Отметим, что при использовании большинства метрик значения признаков надо масштабировать. Условно говоря, чтобы признак \"Зарплата\" с диапазоном значений до 100 тысяч не вносил больший вклад в расстояние, чем \"Возраст\" со значениями до 100. \n",
    " - веса соседей (соседи тестового примера могут входить с разными весами, например, чем дальше пример, тем с меньшим коэффициентом учитывается его \"голос\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример: пусть класс объекта определяется 5-ю ближайшими соседями. К какому классу будут отнесены объекты на месте знаков?\n",
    "\n",
    "К каким классам будут принадлежать неизвестные объекты, если веса всех синих объектов увеличить в 2 раза?\n",
    "\n",
    "Какие способы масштабирования вы помните/знаете?\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Debo_Cheng/publication/293487460/figure/fig1/AS:651874571149316@1532430417078/An-example-of-kNN-classification-task-with-k-5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(всё бы хорошо, ток окружности стереть, иначе не интересно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация kNN\n",
    "Предположим, что каждый объект имеет $d$ признаков. Обучающая выборка состоит из $N$ примеров, а тестовая - из $M$. Давайте попробуем оценить сложность какого-нибудь простого варианта алгоритма $k$ ближайших соседей. Для простоты возьмём евклидову метрику."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что для обучающей и тестовой выборок с 10000 примеров в каждой и 10 признаками на объект, алгоритм может работать достаточно долго. Тем не менее, можно ~~вдрочить~~ ускорить метод $k$ ближайших соседей засчёт некоторых свойств метрики. Подробнее про то, как это работает можно посмотреть <a href=\"https://www.youtube.com/watch?v=UUm4MOyVTnE\">здесь</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Плюсы и минусы метода ближайших соседей\n",
    "\n",
    "Плюсы:\n",
    " - Простая реализация\n",
    " - Можно адаптировать под нужную задачу выбором метрики или ядра (в двух словах: ядро может задавать операцию сходства для сложных объектов типа графов, а сам подход kNN остается тем же). Кстати, профессор ВМК МГУ и опытный участник соревнований по анализу данных Александр Дьяконов любит самый простой kNN, но с настроенной метрикой сходства объектов. Можно почитать про некоторые его решения (в частности, \"VideoLectures.Net Recommender System Challenge\") на персональном [сайте](http://alexanderdyakonov.narod.ru/contests.htm);\n",
    " - Неплохая интерпретация, можно объяснить, почему тестовый пример был классифицирован именно так. Хотя этот аргумент можно атаковать: если число соседей большое, то интерпретация ухудшается (условно: \"мы не дали ему кредит, потому что он похож на 350 клиентов, из которых 70 – плохие, что на 12% больше, чем в среднем по выборке\").\n",
    " \n",
    "Минусы:\n",
    " - Метод считается быстрым в сравнении, например, с композициями алгоритмов, но в реальных задачах, как правило, число соседей, используемых для классификации, будет большим (100-150), и в таком случае алгоритм будет работать не быстро;\n",
    " - Если в наборе данных много признаков, то трудно подобрать подходящие веса и определить, какие признаки не важны для классификации/регрессии;\n",
    " - Зависимость от выбранной метрики расстояния между примерами. Выбор по умолчанию евклидового расстояния чаще всего ничем не обоснован. Можно отыскать хорошее решение перебором параметров, но для большого набора данных это отнимает много времени;\n",
    " - Нет теоретических оснований выбора определенного числа соседей - только перебор (впрочем, чаще всего это верно для всех гиперпараметров всех моделей). В случае малого числа соседей метод чувствителен к выбросам, то есть склонен переобучаться;\n",
    " - Как правило, плохо работает, когда признаков много, из-за \"прояклятия размерности\". Про это хорошо рассказывает известный в ML-сообществе профессор Pedro Domingos – [тут](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) в популярной статье \"A Few Useful Things to Know about Machine Learning\", также \"the curse of dimensionality\" описывается в книге Deep Learning в [главе](http://www.deeplearningbook.org/contents/ml.html) \"Machine Learning basics\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс KNeighborsClassifier в Scikit-learn\n",
    "Основные параметры класса sklearn.neighbors.KNeighborsClassifier:\n",
    " - weights: \"uniform\" (все веса равны), \"distance\" (вес обратно пропорционален расстоянию до тестового примера) или другая определенная пользователем функция\n",
    " - algorithm (опционально): \"brute\", \"ball_tree\", \"KD_tree\", или \"auto\". В первом случае ближайшие соседи для каждого тестового примера считаются перебором обучающей выборки. Во втором и третьем - расстояние между примерами хранятся в дереве, что ускоряет нахождение ближайших соседей. В случае указания параметра \"auto\" подходящий способ нахождения соседей будет выбран автоматически на основе обучающей выборки.\n",
    " - leaf_size (опционально): порог переключения на полный перебор в случае выбора BallTree или KDTree для нахождения соседей\n",
    " - metric: \"minkowski\", \"manhattan\", \"euclidean\", \"chebyshev\" и другие."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST digits\n",
    "До этого момента мы рассматривали задачу бинарной классификации. Сейчас мы будем рассматривать задачу с более чем двумя классами. Как обобщить kNN классификатор на случай многих классов? А что делать с произвольным бинарным классификатором?\n",
    "\n",
    "Наша задача: правильно относить картинки рукописных цифр к одному из 10 классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_digits()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинки здесь представляются матрицей 8 x 8 (интенсивности белого цвета для каждого пикселя). Далее эта матрица \"разворачивается\" в вектор длины 64, получается признаковое описание объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,:].reshape([8,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axes = plt.subplots(1, 4, sharey=True, figsize=(16,6))\n",
    "for i in range(4):    \n",
    "    axes[i].imshow(X[i,:].reshape([8,8]), cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на соотношение классов в выборке, видим, что примерно поровну нулей, единиц, ..., девяток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([178, 182, 177, 183, 181, 182, 181, 179, 174, 180])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим 70% выборки (X_train, y_train) под обучение и 30% будут отложенной выборкой (X_holdout, y_holdout). отложенная выборка никак не будет участвовать в настройке параметров моделей, на ней мы в конце, после этой настройки, оценим качество полученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3, random_state=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "simple_knn = KNeighborsClassifier()\n",
    "simple_knn.fit(X_train, y_train)\n",
    "print(accuracy_score(y_holdout, simple_knn.predict(X_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте улучшим его accuracy на тестовой выборке до ...%, подобрав оптимальные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большая часть материала спизжена с"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "Автор материала: программист-исследователь Mail.ru Group, старший преподаватель Факультета Компьютерных Наук ВШЭ Юрий Кашницкий. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
