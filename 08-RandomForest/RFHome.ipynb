{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AldI23FZmMX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\n",
    "from pandas.testing import assert_frame_equal\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR, DecisionTreeClassifier as DTC\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNR\n",
    "from sklearn.linear_model import LinearRegression as LinReg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvls9GQxWK5O"
   },
   "source": [
    "# SLIDE (1) Bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9POKe84XWK6A"
   },
   "source": [
    "На вход массив чисел $X$ и число бутстрепных выборок $B$. Необходимо реализовать свой бутстреп и найти матожидание и стандартную ошибку у бутстрепных выборок.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X = np.array([37,43,38,36,17,40,40,45,41,84])\n",
    "B = 100000\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "42.1, 4.56\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNTDVikgWK6F"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_awC3d6CWK6I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import sem # ищет SE среднего\n",
    "\n",
    "def get_stats(X: np.array, B:int)->tuple:\n",
    "    SEs = []\n",
    "    means = []\n",
    "    for _ in range(B):\n",
    "        sample = np.random.choice(X, len(X), True)\n",
    "        SEs.append(sem(sample))\n",
    "        means.append(sample.mean())\n",
    "    mean = np.array(means).mean()\n",
    "    SE = np.array(SEs).mean()\n",
    "    return mean, SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.13739\n",
      "4.585872232799194\n",
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "X = np.array([37,43,38,36,17,40,40,45,41,84])\n",
    "B = 10000\n",
    "\n",
    "mean, se = get_stats(X, B)\n",
    "print(mean)\n",
    "print(se)\n",
    "\n",
    "assert np.abs(mean - 42.1) < 0.05\n",
    "assert np.abs(se - 4.56) < 0.03\n",
    "######################################################\n",
    "print('Well Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Bias-variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подается **один** объект $(x, y)$ и список из нескольких **обученных** моделей. \n",
    "\n",
    "Необходимо найти $error$, $bias^2$, $variance$ для данного объекта.\n",
    "\n",
    "Теперь все аккуратно запишем, чтобы не запутаться.\n",
    "\n",
    "* $(x, y)$ - тестировачная выборка\n",
    "* $a_1(\\cdot), \\ldots, a_M(\\cdot)$ - модели (это не обученные на бутстрепе модели, а просто возможные модели из пространства $\\mathbb{A}$, которое мы выбрали)\n",
    "\n",
    "Как настоящие статистики мы можем ~~забить~~ оценить матожидание как среднее.**Это не смешанная модель, а именно оценка матожидания через среднее**\n",
    "$$\\mathbb{E}a(x) = \\frac{1}{M}\\sum_{i=1}^{M}a_i(x)$$\n",
    "\n",
    "**Error** (берем матожидание от квадрата разности)\n",
    "\n",
    "$$error = \\mathbb{E}_{a}(a(x)-y)^2 = \\frac{1}{M}\\sum_{i=1}^{M}(a_i(x) - y)^2$$\n",
    "\n",
    "**Bias** (заметьте, что возвращаем квадрат bias, а не просто bias)\n",
    "\n",
    "$$bias^2 = \\Big(y - \\mathbb{E}_{a}[a(x)]\\Big)^2 = \\Big(y - \\frac{1}{M}\\sum_{i=1}^{M}a_i(x)\\Big)^2$$  \n",
    "\n",
    "\n",
    "**Variance** (ищем смещенную оценку)\n",
    "\n",
    "$$variance = \\mathbb{D}_{a}a(x)= \\mathbb{E}_{a}(a(x) - \\mathbb{E}_{a}a(x))^2 = \\frac{1}{M}\\sum_{i=1}^{M}\\Big(a_i(x)-\\frac{1}{M}\\sum_{r=1}^{M}a_r(x)\\Big)^2$$\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "x, y = np.array([[0,0,0]]), 0\n",
    "estimators = [DecisionTreeRegressor(max_depth=3, random_state=1),  #already fitted estimators\n",
    "              DecisionTreeRegressor(max_depth=5, random_state=1)]\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "error, bias2, var = 3.574, 3.255, 0.319\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bias_variance_decomp(x_test:np.array, y_test:int, estimators:list)->tuple:\n",
    "    error = 0\n",
    "    bias2 = 0\n",
    "    var = 0\n",
    "    est_count = len(estimators)\n",
    "    est_sum = 0\n",
    "    for estimator in estimators:\n",
    "        y_ = estimator.predict(x_test)\n",
    "        est_sum += y_\n",
    "    for estimator in estimators:\n",
    "        y_ = estimator.predict(x_test)\n",
    "        error += pow(y_test - y_, 2)\n",
    "        var += pow(y_ - est_sum / est_count, 2)\n",
    "    error /= est_count\n",
    "    bias2 = pow(y_test - est_sum / est_count, 2)\n",
    "    var = var / est_count\n",
    "    return error[0], bias2[0], var[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done\n"
     ]
    }
   ],
   "source": [
    "def generate(n_samples, noise, f):\n",
    "    X = np.linspace(-4, 4, n_samples)\n",
    "    y = f(X)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "######################################################\n",
    "\n",
    "n_train = 150        \n",
    "noise = 0.1\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "X, y = generate(n_samples=n_train, noise=noise, f=f)\n",
    "\n",
    "estimators = [DTR(max_depth=2, random_state=1).fit(X, y), \n",
    "              DTR(max_depth=4, random_state=1).fit(X, y)]\n",
    "\n",
    "x, y = np.array([[2]]), 1.5\n",
    "\n",
    "error, bias, var = bias_variance_decomp(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([0.108, 0.083, 0.025]), decimal=3)\n",
    "\n",
    "x, y = np.array([[-0.7]]), 0.8\n",
    "error, bias, var = bias_variance_decomp(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([0.045, 0.002, 0.043]), decimal=3)\n",
    "\n",
    "######################################################\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=3, n_informative=3, bias=2, noise=10, \n",
    "                       n_targets=1, shuffle=False, random_state=10)\n",
    "\n",
    "estimators = [DTR(max_depth=3, random_state=1).fit(X, y), \n",
    "              DTR(max_depth=5, random_state=1).fit(X, y)]\n",
    "\n",
    "x, y = np.array([[0,0,0]]), 0\n",
    "error, bias, var = bias_variance_decomp(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([3.574, 3.255, 0.319]), decimal=3)\n",
    "\n",
    "print('Well Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Bias-variance v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь тоже самое, только для нескольких объектов\n",
    "\n",
    "На вход подается тестовая выборка объект $(X_test, y_test)$ и список из нескольких **обученных** моделей. \n",
    "\n",
    "Необходимо найти $error$, $bias^2$, $variance$, $noise$ для данного объекта.\n",
    "\n",
    "$$error = \\mathbb{E}_{x,y}\\mathbb{E}_{a}(a(x)-y)^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{M}\\sum_{j=1}^{M}(a_j(x_i) - y_i)^2$$\n",
    "\n",
    "$$bias^2 = \\mathbb{E}_{x,y}\\Big(y - \\mathbb{E}_{a}[a(x)]\\Big)^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\Big(y_i - \\frac{1}{M}\\sum_{j=1}^{M}a_j(x_i)\\Big)^2$$  \n",
    "\n",
    "$$variance = \\mathbb{E}_{x,y}\\mathbb{D}_{a}a(x)= \\mathbb{E}_{x,y}\\mathbb{E}_{a}(a(x) - \\mathbb{E}_{a}a(x))^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{M}\\sum_{j=1}^{M}\\Big(a_j(x_i)-\\frac{1}{M}\\sum_{r=1}^{M}a_r(x_i)\\Big)^2$$\n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "x = np.array([[  0,   0,   0],\n",
    "              [0.1, 0.1, 0.1]])\n",
    "y = np.array([0, 0.1])\n",
    "\n",
    "estimators = [DecisionTreeRegressor(max_depth=3, random_state=3), \n",
    "              DecisionTreeRegressor(max_depth=5, random_state=3)]\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "error, bias2, var = 3.399, 3.079, 0.319\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bias_variance_decomp2(x_test:np.array, y_test:np.array, estimators:list)->tuple:\n",
    "    error = 0\n",
    "    bias2 = 0\n",
    "    var = 0\n",
    "    obj_count = x_test.shape[0]\n",
    "    est_count = len(estimators)\n",
    "    est_mean = 0\n",
    "    for i in range(obj_count):\n",
    "        x = x_test[i].reshape(1, -1)\n",
    "        y = y_test[i]\n",
    "        bias = 0\n",
    "        for estimator in estimators:\n",
    "            prediction = estimator.predict(x)\n",
    "            error += pow(prediction - y, 2)\n",
    "            bias += prediction\n",
    "            est_mean = 0\n",
    "            for est in estimators:\n",
    "                est_mean += est.predict(x)\n",
    "            est_mean /= est_count\n",
    "            var += pow(prediction - est_mean, 2)\n",
    "        bias /= est_count\n",
    "        bias = pow(y - bias, 2)\n",
    "        bias2 += bias\n",
    "    error /= est_count\n",
    "    error /= obj_count\n",
    "    bias2 /= obj_count\n",
    "    var /= est_count\n",
    "    var /= obj_count\n",
    "    return error[0], bias2[0], var[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]]\n",
      "[[-0.7]]\n",
      "[[0 0 0]]\n",
      "[[0. 0. 0.]]\n",
      "[[0.1 0.1 0.1]]\n",
      "Well Done\n"
     ]
    }
   ],
   "source": [
    "def generate(n_samples, noise, f):\n",
    "    X = np.linspace(-4, 4, n_samples)\n",
    "    y = f(X)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "######################################################\n",
    "\n",
    "n_train = 150        \n",
    "noise = 0.1\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "\n",
    "X, y = generate(n_samples=n_train, noise=noise, f=f)\n",
    "\n",
    "estimators = [DTR(max_depth=2, random_state=1).fit(X, y), \n",
    "              DTR(max_depth=4, random_state=1).fit(X, y)]\n",
    "\n",
    "x = np.array([[2], [-0.7]]) \n",
    "y = np.array([1.5, 0.8])\n",
    "\n",
    "error, bias, var = bias_variance_decomp2(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          (np.array([0.108, 0.083, 0.025]) + np.array([0.045, 0.002, 0.043])) / 2, decimal=3)\n",
    "\n",
    "######################################################\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=3, n_informative=3, bias=2, noise=10, \n",
    "                       n_targets=1, shuffle=False, random_state=10)\n",
    "\n",
    "estimators = [DTR(max_depth=3, random_state=1).fit(X, y), \n",
    "              DTR(max_depth=5, random_state=1).fit(X, y)]\n",
    "\n",
    "x = np.array([[  0,   0,   0]])\n",
    "y = np.array([0])\n",
    "\n",
    "error, bias, var = bias_variance_decomp2(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([3.574, 3.255, 0.319]), decimal=3)\n",
    "\n",
    "\n",
    "x = np.array([[  0,   0,   0],\n",
    "              [0.1, 0.1, 0.1]])\n",
    "y = np.array([0, 0.1])\n",
    "\n",
    "error, bias, var = bias_variance_decomp2(x, y, estimators)\n",
    "\n",
    "assert_array_almost_equal(np.array([error, bias, var]), \n",
    "                          np.array([3.399, 3.079, 0.319]), decimal=3)\n",
    "\n",
    "print('Well Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подается некий **необученный** алгоритм регрессии, тренировачная и тестовые выборки и число бутстрепных выборок. Необходимо \n",
    "* бустингом сделать несколько выборок $X_1, \\ldots, X_B$\n",
    "* обучить несколько алгоритмов на этих выборках: $a_1(\\cdot), \\ldots, a_B(\\cdot)$\n",
    "* реализовать бэггинг этого алгоритма и найти собственно предсказания, $error$, $bias^2$ и $variance$.\n",
    "\n",
    "Вот теперь аккуратно. Это - **не матожидание**! Это модель такая.\n",
    "$$a(x) = \\frac{1}{B}\\sum_{b=1}^{B}a_b(x)$$\n",
    "\n",
    "А вот ее матожидание равно для всех алгоритмов:\n",
    "$$\\mathbb{E}_aa(x) = \\mathbb{E}_a\\frac{1}{B}\\sum_{b=1}^{B}a_b(x) = \\mathbb{E}_aa_1(x)$$\n",
    "\n",
    "Но так как теперь, нам нужно посчитать матожидание, мы воспользуемся нашим множеством алгоритмов, обученных на бутстрепе, чтобы получить оценку матожидания единичного алгоритма.\n",
    "\n",
    "$$\\mathbb{E}_aa_1(x) = \\frac{1}{B}\\sum_{j=1}^{B}a_j(x)$$\n",
    "\n",
    "Остальные формулы берутся из предыдущей задачи.\n",
    "\n",
    "P.S.\n",
    "* Так как тут есть вероятности, в целом тесты могут `редко` не взлететь. Перезашлите задачу в этом случае.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "estimator = DecisionTreeRegressor(max_depth=1)\n",
    "X_train = np.array([[1, 1], [2, 2]])\n",
    "y_train = np.array([1, 2])\n",
    "X_test  = np.array([[0, 0], [4, 4], [8, 8]])\n",
    "y_test  = np.array([0, 4, 8])\n",
    "\n",
    "B = 10\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([3.708, 6.016])\n",
    "error  = 3.5 \n",
    "bias^2 = 0.1\n",
    "var    = 3.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "def bagging(estimator, X_train, y_train, X_test, y_test, boot_count):\n",
    "    obj_count = X_train.shape[0]\n",
    "    X_samples, y_samples = get_samples(X_train, y_train, boot_count)\n",
    "    \n",
    "    estimators = []\n",
    "    predicts = []\n",
    "    \n",
    "    for i in range(boot_count):\n",
    "        new_est = clone(estimator)\n",
    "        new_est.fit(X_samples[i], y_samples[i])\n",
    "        predict = new_est.predict(X_test)\n",
    "        predicts.append(predict)\n",
    "        estimators.append(new_est)\n",
    "\n",
    "    predicts = np.array(predicts)\n",
    "    n = predicts.shape[0]\n",
    "    y_pred = predicts.sum(axis=0)/n\n",
    "    \n",
    "    loss = 0\n",
    "    bias = 0\n",
    "    var = 0\n",
    "    \n",
    "    m = y_test.shape[0]\n",
    "    \n",
    "    predictions = {}\n",
    "    for i in range(m):\n",
    "        x = np.array([X_test[i]])\n",
    "        predictions[i] = {}\n",
    "        for j in range(n):\n",
    "            pred = estimators[j].predict(x)\n",
    "            predictions[i][j] = pred\n",
    "    \n",
    "    for predict in predicts:\n",
    "        temp = 0\n",
    "        for i in range(m):\n",
    "            temp += (predict[i] - y_test[i])**2\n",
    "        temp /= m\n",
    "        loss += temp    \n",
    "    loss /= n\n",
    "    \n",
    "    for i in range(m):\n",
    "        yt = y_test[i]\n",
    "        x = X_test[i]\n",
    "        temp = 0\n",
    "        for j in range(boot_count):\n",
    "            pred =  predictions[i][j]\n",
    "            temp += pred\n",
    "        temp /= boot_count\n",
    "        bias = bias + (yt - temp)**2\n",
    "    bias /= m\n",
    "    \n",
    "    for i in range(m):\n",
    "        temp = 0\n",
    "        x = X_test[i]\n",
    "        for j in range(n):\n",
    "            pred = predictions[i][j]\n",
    "            temp2 = 0\n",
    "            for r in range(n):\n",
    "                temp2 += predictions[i][r]\n",
    "            temp2 /= n\n",
    "            temp = temp + (pred - temp2)**2\n",
    "        temp /= n\n",
    "        var += temp\n",
    "    var /= m\n",
    "    return y_pred, loss, bias, var\n",
    "\n",
    "def get_samples(X_test, y_test, boot_count):\n",
    "    obj_count = X_train.shape[0]\n",
    "    X_samples = []\n",
    "    y_samples = []\n",
    "    for _ in range(boot_count):\n",
    "        indices = np.random.choice(obj_count, obj_count, True)\n",
    "        X_sample = []\n",
    "        y_sample = []\n",
    "        for index in indices:\n",
    "            X_sample.append(X_test[index])\n",
    "            y_sample.append(y_test[index])\n",
    "        X_samples.append(X_sample)\n",
    "        y_samples.append(y_sample)\n",
    "    return X_samples, y_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.65675]\n",
      "[17.56135542]\n",
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "estimator = DTR(max_depth=2)\n",
    "X_train = np.array([[0, 0], [1, 1], [5, 5], [8, 8], [10, 10]])\n",
    "y_train = np.array([0, 1, 5, 8, 10])\n",
    "X_test  = np.array([[4, 4], [6, 6]])\n",
    "y_test  = np.array([4, 6])\n",
    "\n",
    "B = 100\n",
    "\n",
    "y_pred, loss, bias, var = bagging(estimator, X_train, y_train, X_test, y_test, boot_count=B)\n",
    "\n",
    "# Да я в курсе что очень грубые ограничения, просто пример игрушечный на таком малом количестве данных\n",
    "assert_array_almost_equal(y_pred, np.array([4, 6]), decimal=0)\n",
    "\n",
    "assert_almost_equal(loss, 3.7, decimal=0) \n",
    "assert_almost_equal(bias, 0.1, decimal=1) \n",
    "assert_almost_equal(var,  3.7, decimal=0) \n",
    "\n",
    "######################################################\n",
    "B = 10\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "tree = DTR(max_depth=7)\n",
    "\n",
    "y_pred, loss, bias, var = bagging(\n",
    "        tree, X_train, y_train, X_test, y_test, boot_count=200)\n",
    "\n",
    "assert_almost_equal(loss, 32, decimal=0) \n",
    "assert_almost_equal(bias, 14, decimal=0) \n",
    "assert_almost_equal(var,  18, decimal=0) \n",
    "\n",
    "print('Well Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) RF Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось переделать чуток предыдущую задачу в `RandomForest`. \n",
    "Но теперь мы наконец попробуем классификацию. (Пока только бинарную)\n",
    "\n",
    "План\n",
    "* Также делаем бутстрепные выборки\n",
    "* Бэггинг теперь будет только по деревьям классификации\n",
    "* Будем передавать параметр `n_estimators`, `max_depth` и `max_features`\n",
    "\n",
    "Как выбирать ответ в задаче классификации?\n",
    "* Для каждого внутреннего дерева решений находим веротности обоих классов для каждого объекта $X_test$:\n",
    "  * Вызываем `predict_proba` у `DecisionTreeClassifier`\n",
    "* Усредняем вероятности класса и объекта по деревьям:\n",
    "  * $P(n_{class}=d, object=x_k) = \\frac{1}{B}\\sum_{i=1}^{B}P(n_{class}=d, object=x_k, tree=b_i)$\n",
    "* Для каждого объекта выбираем тот класс, у которого выше вероятность\n",
    "\n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[0, 0], [4, 4], [5, 5], [10, 10]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "X_test  = np.array([[3, 3], [6, 6]])\n",
    "y_test  = np.array([0, 1])\n",
    "\n",
    "B = 1000\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "model.predict(X_test) == np.array([0, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "class MyRFC():\n",
    "    def __init__(self, n_estimators=10, max_features=None, max_depth=None):\n",
    "        self.n = n_estimators\n",
    "        self.estimators_ = []\n",
    "        for _ in range(n_estimators):\n",
    "            self.estimators_.append(DTC(max_depth=max_depth, max_features=max_features))\n",
    "        \n",
    "    def fit(self, X_train: np.array, y_train: np.array):\n",
    "        X_samples, y_samples = self._get_samples(X_train, y_train, self.n)\n",
    "        for i in range(self.n):\n",
    "            self.estimators_[i].fit(X_samples[i], y_samples[i])\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X_test) -> np.array:\n",
    "        n = X_test.shape[0]\n",
    "        probas = []\n",
    "        for i in range(self.n):\n",
    "            proba = self.estimators_[i].predict_proba(X_test)\n",
    "            if proba.shape[1] == 1:\n",
    "                b = np.zeros((n,2))\n",
    "                b[:,:-1] = proba\n",
    "                proba = b\n",
    "            probas.append(proba)\n",
    "        me = np.mean(probas, axis=0)\n",
    "        result = []\n",
    "        for elem in me:\n",
    "            if elem[0] >= 0.5:\n",
    "                result.append(0)\n",
    "            else:\n",
    "                result.append(1)\n",
    "        return result\n",
    "    \n",
    "    def predict_proba(self, X_test)-> np.array:\n",
    "        n = X_test.shape[0]\n",
    "        probas = []\n",
    "        for i in range(self.n):\n",
    "            proba = self.estimators_[i].predict_proba(X_test)\n",
    "            if proba.shape[1] == 1:\n",
    "                b = np.zeros((n,2))\n",
    "                b[:,:-1] = proba\n",
    "                proba = b\n",
    "            probas.append(proba)\n",
    "        me = np.mean(probas, axis=0)\n",
    "        return me\n",
    "    \n",
    "    def _get_samples(self, X_test, y_test, boot_count):\n",
    "        obj_count = X_train.shape[0]\n",
    "        X_samples = []\n",
    "        y_samples = []\n",
    "        for _ in range(boot_count):\n",
    "            indices = np.random.choice(obj_count, obj_count, True)\n",
    "            X_sample = []\n",
    "            y_sample = []\n",
    "            for index in indices:\n",
    "                X_sample.append(X_test[index])\n",
    "                y_sample.append(y_test[index])\n",
    "            X_samples.append(X_sample)\n",
    "            y_samples.append(y_sample)\n",
    "        return X_samples, y_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old proba  [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "new proba [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 6 decimals\n\nMismatched elements: 1 / 3 (33.3%)\nMax absolute difference: 1\nMax relative difference: 1.\n x: array([0, 0, 0])\n y: array([0, 1, 0])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-71debf36b054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_pred_my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyRFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0massert_array_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m######################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgauss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_almost_equal\u001b[0;34m(x, y, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n\u001b[1;32m   1046\u001b[0m              \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Arrays are not almost equal to %d decimals'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m              precision=decimal)\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    844\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n\nMismatched elements: 1 / 3 (33.3%)\nMax absolute difference: 1\nMax relative difference: 1.\n x: array([0, 0, 0])\n y: array([0, 1, 0])"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "X_train = np.array([[0, 0], [4, 4], [5, 5], [10, 10]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "X_test  = np.array([[3, 3], [6, 6], [2, 2]])\n",
    "y_test  = np.array([0, 1, 0])\n",
    "\n",
    "B = 1000\n",
    "\n",
    "y_pred_my = MyRFC(n_estimators = 2, max_depth=3).fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "assert_array_almost_equal(y_pred_my, np.array([0, 1, 0]))\n",
    "######################################################\n",
    "from random import gauss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "num_samples = 1000\n",
    "theta = np.linspace(0, 2*np.pi, num_samples)\n",
    "\n",
    "r1 = 1\n",
    "r2 = 2\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "circle = np.hstack([np.cos(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 8), \n",
    "                    np.sin(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 8)])\n",
    "lil = r1 * circle\n",
    "big = r2 * circle\n",
    "X = np.vstack([lil, big])\n",
    "y = np.hstack([np.zeros(num_samples), np.ones(num_samples)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "y_test = y_test.astype('int')\n",
    "\n",
    "\n",
    "y_pred_my = MyRFC(n_estimators = 100, \n",
    "                  max_depth=1).fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "assert accuracy_score(y_pred_my, y_test) > 0.85\n",
    "print('Well Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просто верните отсортированный массив важности фич, полученные из обученного RandomForest. Фичи нумеруются с 1.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X = np.array([[0, 0], [0,1], [1, 0], [1, 1]])\n",
    "y = np.array([0,0,1,1])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "features= np.array([1, 2])\n",
    "importance = np.array([0.75, 0.25])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "def feature_importance(X, y):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X, y)\n",
    "    importance = rf.feature_importances_\n",
    "    positions = [i+1 for i in range(len(importance))]\n",
    "    my_dict = dict(zip(importance, positions))\n",
    "    sorted_importance = sorted(importance, reverse=True)\n",
    "    features = list(map(lambda x: my_dict[x], sorted_importance))\n",
    "    return features, sorted_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([2, 3, 4, 1], [0.4965524985732423, 0.23414282102983167, 0.16000738956112476, 0.10929729083580123])\n",
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression, make_classification\n",
    "\n",
    "######################################################\n",
    "X = np.array([[0, 0], [0,1], [1, 0], [1, 1]])\n",
    "y = np.array([0,0,1,1])\n",
    "\n",
    "f, i = feature_importance(X, y)\n",
    "\n",
    "assert_array_equal(f , np.array([1, 2]))\n",
    "assert i[0] > 0.74\n",
    "######################################################\n",
    "X, y = make_classification(n_samples=1000, \n",
    "                           n_features=4,\n",
    "                           n_informative=2,\n",
    "                           shuffle=False, \n",
    "                           random_state=10)\n",
    "print(feature_importance(X, y))\n",
    "n = 10\n",
    "a = np.zeros((n, X.shape[1]))\n",
    "for i in range(n):\n",
    "    a[i], _ = feature_importance(X, y) \n",
    "\n",
    "assert_array_equal(np.round(a.mean(axis=0)), np.array([2,3,4,1]))\n",
    "\n",
    "######################################################\n",
    "print('Well Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
